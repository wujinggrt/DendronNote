---
id: fbbhwt128xaojy66nxffgv0
title: HumanUp_站立策略
desc: ''
updated: 1742393726482
created: 1742189717381
---

## 1. 作者团队信息
- 作者: Xialin He, Runpei Dong, Zixuan Chen, Saurabh Gupta
- 团队: 来自伊利诺伊大学厄巴纳-香槟分校（University of Illinois Urbana-Champaign）和西蒙弗雷泽大学（Simon Fraser University）

## 2. 背景和动机
- 背景: 人形机器人在实际应用中容易摔倒，且需要人类干预才能恢复。自动摔倒恢复是人形机器人可靠部署的关键前提。手动设计控制器来应对不同的摔倒姿势和复杂地形非常困难。
- 动机: 现有的基于模型的控制方法在应对新环境时泛化能力有限，而基于学习的方法在四足和双足机器人 locomotion 任务中取得了显著进展。然而，摔倒恢复任务涉及复杂的接触模式，需要准确建模碰撞几何和稀疏奖励，这使得直接应用现有方法效果不佳。

## 3. 相关研究
- 人形机器人控制: 包括基于零力矩点（ZMP）的模型控制、优化方法和模型预测控制（MPC）。近年来，基于学习的方法在 locomotion 任务中取得了显著进展。
- 腿部机器人摔倒恢复: 包括通过运动规划、手动设计的轨迹和基于强化学习（RL）的方法。现有方法在处理复杂地形和不可预测的初始状态时存在局限性。
- 角色动画中的摔倒恢复: 一些研究探索了基于 RL 的运动模仿算法，但这些方法通常使用简化动力学，难以直接迁移到真实机器人。

## 4. 核心思路
- 核心问题: 学习人形机器人在不同初始姿势和地形下的摔倒恢复策略。
- 解决方案: 提出一个两阶段的强化学习框架，第一阶段在简化条件下发现恢复动作，第二阶段在强控制正则化下将动作优化为可部署的策略。

## 5. 方案与技术
- 两阶段训练:
    - 第一阶段（发现策略）: 在简化条件下（弱正则化、固定初始姿势、简单碰撞几何等较少约束情况下）训练策略，发现恢复动作。这一阶段主要关注任务完成，比如增加机器人的高度，保持身体直立。限制要求少，目的在于尽可能的探索起身动作。但是还不能够起立，
    - 第二阶段（可部署策略）: 在强控制正则化下（平滑性奖励、关节速度惩罚等）训练策略，跟踪第一阶段发现的动作轨迹，并进行地形和初始姿势的随机化。第一阶段的策略可能会导致电机出问题，第二阶段训练时添加了部署限制来纠正，以训练出更安全的策略。
- 课程学习 (curriculum learning)：直接学习最终的策略难度较大，所以使用逐步逼近的方式。
- 策略架构: 使用多层感知机（MLP）作为策略模型，通过近端策略优化（PPO）进行训练。
- 奖励设计: 包括任务奖励（如高度奖励、对称性奖励）和正则化奖励（如平滑性奖励、关节速度惩罚）。

考虑两个躺下的初始状态，脸部朝上和脸部朝下。为了解决这两个任务，一阶段时，策略应当决定发现站立还是翻身动作。策略此时还未考虑部署的限制，仅考虑完成任务和奖励。第二阶段，需要训练可以部署的策略，需要完成站立或翻身，并且在阶段一的情况下，添加了强控制正则。

观察状态使用编码后的环境外部潜在信息，当前和过去 10 步的自我状态信息。自我状态包含机器人的 roll, pitch, 角速度, DoF 速度, DoF 位置。这些内容在现实世界能够精确获取。使用 PPO 训练 MLPs。在长度为 T 的 episode 中，最大化的 γ-discounted policy 的期望，即 $\mathbb{E}[\Sigma_{t=1}^{T}{\gamma^{t-1}r_t}]$, t 代表 timestep。

### 第一阶段

#### 站立的奖励

奖励设置为 $r_{up}=r_{height}+r_{\Delta{height}}+r_{uprightness}+r_{stand\_on\_feet}+r_{\Delta{feet\_contact\_forces}}+r_{symmetry}$
- $r_{height}$ 激励机器人达到与站立时相同的高度
- $r_{\Delta{height}}$ 激励机器人持续增加高度
- $r_{uprightness}$ 激励机器人增加 z 轴方向上的投影重力，以便直立
- $r_{stand\_on\_feet}$ 激励双腿站立
- $r_{\Delta{feet\_contact\_forces}}$ 激励双脚持续增加接触力
- $r_{symmetry}$ 鼓励（但不要求）机器人输出双侧对称动作，来减少搜索空间和提高学习效率，保持动作自然性和安全，避免起身时单侧用力。

#### 翻身的奖励

$r_{roll}=r_{gravity}$，鼓励机器人变换身体朝向。

### 第二阶段

此阶段训练的策略，将会直接部署到实机。训练一个比第一阶段慢 8 倍的版本。添加一些强正则，保证 Sim2Real 的迁移性。此外，引入典型控制正则奖励。

#### Tracking rewards

此奖励确保机器人精确地跟踪第一阶段发现的运动轨迹，并转化为适合现实世界部署的平滑、安全的动作。

鼓励机器人尽可能模仿发现的动作。有 $r_{tracking} = r_{tracking\_DoF} + r_{tracking\_body}$，解释如下：
- $r_{tracking\_DoF}$ 鼓励机器人移动到参与运动相同的关节位置（DoF 位置），也就是关节要移动到第一阶段发现的轨迹的关节处
- $r_{tracking\_body}$ 鼓励机器人将身体移动到与参考运动相同的位置

#### Stage I to Stage II Curriculum

两阶段完成的工作，实际上是一个课程学习，从易到难的工作。分为两阶段训练十分重要。特别地，阶段一步入阶段二时，复杂程度会增加。主要在以下方面：
1. 碰撞模型（Collision Mesh）
    - Stage I：使用简化的碰撞模型，以加速运动发现过程。
    - Stage II：切换到完整的碰撞模型，以提高Sim-to-Real的性能和现实世界中的适用性。
2. 初始姿势随机化（Posture Randomization）
    - Stage I：从规范的初始姿势开始学习，以加速学习过程。
    - Stage II：使用随机化的初始姿势进行训练，以提高策略对不同初始条件的泛化能力。
3. 控制正则化和环境随机化（Control Regularization and Domain Randomization）
    - Stage I：采用较弱的控制正则化，允许发现快速但不安全的起身动作。
    - Stage II：引入强控制正则化，确保动作的平滑性和安全性。同时，增加环境随机化，如地形随机化和噪声注入，以提高策略的鲁棒性和适应性。

弱到强的控制正则化（Weak to Strong Control Regularization）
    - 弱控制正则化（Weak Control Regularization）：在第一阶段，使用较弱的控制正则化，允许机器人探索快速但可能不安全的起身动作。这有助于在较少约束的条件下发现有效的运动轨迹。
    - 强控制正则化（Strong Control Regularization）：在第二阶段，引入强控制正则化，确保动作的平滑性和安全性。这包括使用平滑性奖励、关节速度惩罚等，以确保动作适合现实世界部署。
快速到慢速的动作速度（Fast to Slow Motion Speed）
    - 快速动作（Fast Motion）：在第一阶段，发现的运动轨迹速度较快，但可能不适用于现实世界部署。
    - 慢速动作（Slow Motion）：在第二阶段，将发现的运动轨迹放慢（例如，起身动作从1秒放慢到8秒），以提供更稳定的跟踪目标，更好地适应现实世界的控制要求。
固定到随机的动力学和领域参数（Fixed to Random Dynamics and Domain Parameters）
    - 固定参数（Fixed Parameters）：在第一阶段，使用固定的环境参数，如固定的地形和动力学参数，以简化学习过程。
    - 随机参数（Random Parameters）：在第二阶段，引入环境随机化，如地形随机化和噪声注入，以提高策略的鲁棒性和适应性。这有助于机器人在多种不同的现实世界环境中表现良好。

## 6. 实验与结论
- 实验设置: 在仿真和真实世界中使用 Unitree G1 人形机器人进行实验，测试了从仰卧和俯卧姿势恢复的能力。
- 实验结果: 提出的方法在仿真和真实世界中均表现出色，能够在不同地形和初始姿势下成功恢复。与基线方法相比，HumanUP 在任务成功率、动作平滑性和安全性方面表现更好。
- 结论: 两阶段训练方法有效解决了复杂接触模式下的摔倒恢复问题，成功实现了从仿真到真实世界的迁移。

## 7. 贡献
- 提出了一种两阶段的强化学习框架，用于学习人形机器人在不同初始姿势和地形下的摔倒恢复策略。
- 首次在真实世界中成功演示了 人形机器人从仰卧和俯卧姿势恢复的能力。
- 通过实验验证了 所提出方法在仿真和真实世界中的有效性和鲁棒性。

## 8. 不足
- 依赖高性能物理仿真平台，如 IsaacGym，当前机器人仿真器在接触动力学模拟的准确性和效率上仍有局限。
- 强化学习过程是一个欠定问题，难以确保学习到的动作完全符合人类行为，例如，学习到的动作可能会举起双手以保持平衡。

## QA

> Q：如果脸部朝下，那么，第一阶段和第二阶段训练的内容是翻身，而不是站立，可以这样理解吗？

在HUMANUP框架中，第一阶段和第二阶段的训练内容确实会根据机器人初始姿势的不同而有所变化：
- 第一阶段：当机器人处于脸部朝下的姿势（俯卧姿势）时，第一阶段的训练主要是探索如何翻身，即从俯卧姿势翻转到仰卧姿势。这个阶段的目标是发现能够成功翻身的运动轨迹，而不受平滑性或扭矩限制的约束。奖励函数主要关注任务完成，例如鼓励机器人改变身体方向，使其投影重力接近仰卧时的目标投影重力。
- 第二阶段：在机器人通过第一阶段学会了如何翻身之后，第二阶段的训练会将翻身动作进一步优化，使其更加平滑、缓慢和安全，以便能够直接部署到现实世界中。这个阶段的训练会加入强控制正则化，以确保动作的平滑性和安全性。

所以，当机器人处于脸部朝下的姿势时，第一阶段和第二阶段的训练内容主要是围绕如何翻身展开的。

## 专项学习

关于某个 reward 一致没有提升，是否可以加大 scale 的参数，放大影响，学习好指定奖励。

## 站立的奖励

如果与时序有关，比如先附身，后挺直。那么时序上，如何设置奖励？与 episode_length_buf 中的值，要相关吗？用来判断，是否在几个时间步内，要有这个动作？

重心应当在大腿上部。就像深蹲时一样。

## Code Base

在 train 脚本中，指定的 proj_name 为 ${robot_name}_up，比如传入 bash run.sh g1waist stage1_get_up cuda:0，robot_name 代表 g1waist，则 task_name 和 proj_name 都是 g1waist_up。

日志在 logs 目录下，按照 proj_name 细分为训练的目录，权重、日志文件保存在 logs/${proj_name} 下。

评估，eval 时，指定的 checkpoint 为 int 类型，加载 logs/${proj_name}/${exptid}/model_${checkpoint}.pt，如果指定 -1 则加载最后的模型。

## Ref and Tag

[humanoid-getup](https://humanoid-getup.github.io/)