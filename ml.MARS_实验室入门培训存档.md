---
id: p6ccth43jm2xccxbvshzknz
title: MARS_实验室入门培训存档
desc: ''
updated: 1741233712545
created: 1741233429362
---

MARS实验室入门培训👍
## 前言
培训内容包含从AI入门到科研入门，涵盖独立完成科研任务的全部内容，完成将会体验和学习到完整的科研流程。组里涵盖方向很多，一定能找到你感兴趣的方向。给出方向在实验室内已有良好基础和背景，且有丰富的计算资源，学长学姐可以帮忙答疑解惑。MARS实验室主页，进实验室没有时间限制，我们欢迎任何年级和任何专业的朋友，在这里激发你的创造力，Make something interesting！Enjoy Yourself Here🤗🤗🤗

本科生亮点成果：
- 论文：2022年以来，已有本科生发表CCF-A类论文数: 12篇（一作），大部分在大二和大三发表，包含NeurIPS 2024，CVPR 2024，ICML 2024， AAAI 2024，ECCV 2024，NeurIPS 2023，IJCAI 2023，ACM MM 2022 等
- 竞赛：中国大学生计算机设计大赛全国一等奖、全国人工智能学会应用场景挑战赛专项一等奖、蓝桥杯全国二等奖等，国际计算机视觉顶会ICCV 2021赛道冠军
- 荣誉：国家级大创、CCF优秀大学生、CS珞珈计科先锋、商汤奖学金等

本科生科研主要培养目标，你将收获以下内容：
- 了解人工智能基础知识，读懂前沿算法代码，学会运用前沿技术解决问题
- 掌握科研基本流程，锻炼严谨的科研思维，熟练运用各类科研工具和文献技巧
- 探索创新性Idea并实现，在人工智能顶级会议上发表论文，参与国内外学术会议交流学习
- 转换科研项目成果（国家级大创），斩获各种竞赛奖项，大厂名组访问实习

本次入门培训时长一个月，希望能在培训任务的指引下，带领你掌握以下内容：
- 深度学习/机器学习的基本原理：数据集，模型，训练原理等；重点掌握计算机视觉方向的常见任务；
- 深度学习代码的基本功：对Python、PyTorch有基本的了解，清楚Anaconda的作用，能使用Linux系统的远程服务器进行深度学习任务；能在给出GitHub仓库的情况下，根据论文内容跑通代码；能对已有代码进行改进；
- 科研基础：了解什么是arXiv、如何进行文献管理和阅读(Zotero)、如何做文献笔记和论文调研报告；
- 计算机视觉：了解计算机视觉的基本原理，了解诸如分类模型中的AlexNet、ResNet等经典卷积神经网络模型，了解以Transformer技术为基础的视觉模型ViT及其后续衍生，能够简单、基本的视觉任务训练及测试Pipeline（手写数字识别、检测等...）了解自监督学习概念，对比学习技术。了解模型压缩的概念以及知识蒸馏技术。
- 基本的PPT制作技能：可以考虑使用Powerpoint，或者LaTeX的Beamer包完成汇报PPT的制作

## 入门培训内容（一个月左右，主要考核是否对科研是否有兴趣，愿意投入）
1. 深度学习的基本原理和代码功底（一周左右）：
    1. 参考李宏毅的机器学习课程，学习机器学习、深度学习的基本原理。
        1. 只需学习2/18，2/25，3/04，3/25，4/22，4/29，5/06这几日的课程，其余课程内容目前不用学习。在听课程内容时，优先听Class Material，其余的Preparation - zh和Extra Material作为补充；4/22，4/29，5/06的课程只听Preparation - zh；
        2. 非常建议完成布置的作业并听助教的HW讲解，强烈建议在5天之内完成这个课程。
    2. 你可以参考这个CS231N专栏，学习计算机视觉的基本原理。这个教程和第一个有一些重复，因此可以主学一个，然后快速过一遍这个专栏来查漏补缺。强烈推荐看完9、12、13和16。
    3. 如果对Python语言没有太多了解，可以先学习Python基础教程，也可以学习https://cs231n.github.io/python-numpy-tutorial/和Jupyter Notebook教程。对Linux不熟练的，学习Linux基础教程。对Git不熟悉的，学习一小时Git教程。
    4. PyTorch的学习，建议在掌握理论知识，听完了李宏毅老师的课程的基础上，学习PyTorch官方文档给出的教程和几个示例。同时，强烈推荐学习小土堆的教程（如果想快点上手，重点推荐这个教程）。
    5. 对LaTeX不熟悉的同学请学习LaTeX基础教程。
2. 科研基础（1天左右）：通过网络搜索，了解什么是arXiv、什么是Overleaf，并初步了解如何使用Zotero等文献管理软件，如何做文献笔记和论文调研报告，了解PapersWithCode, Google Scholar；学习使用Powerpoint，或者可选LaTeX的Beamer包完成汇报PPT的制作；了解BibTeX，知道参考文献的几种写法和格式；
3. 文献阅读（5-20天左右）：这部分主要是为你打下AI的研究基础。你可以选择行人重识别、联邦学习或者多模态学习来作为主要方向，需要阅读一些相关领域的经典论文，复现其中的一些未来常用的方法论文，并完成一份文献阅读报告，为自己未来在研究生期间的工作打下坚实的基础。
## 入门考核内容
友情提示：这个不是任务，与其说是考核，不如说是我们更想让你更加深入理解科研的组成部分，明白如何进入到科研的模式中。我们不希望出现为了应付大作业式GPT的思考，你的内容可以具体或者天马行空，但不要泛泛而谈。字数、内容都不作为好坏的标准，我们希望能够这个过程让你真正提升的，不仅知识水平，还有自学能力。如果你已经部分了解第一、二、三章所涉及到的内容，可以和导师联系，我们会安排一次面聊对其中涉及到的基础知识进行询问，通过者可以跳过以下内容。
1. 内容包括但不限于：
    1. 第一章：基础知识。内容应当简明扼要，主要谈理解，拒绝从网上直接复制粘贴；这部分内容不超过5页；
        1. 行人重识别：了解有监督和无监督的行人重识别基础概念和研究背景[23]，以及延伸出来的跨模态行人重识别（可见光-红外[26][27]、可见光-素描[31]、图像-文本[28]等多种跨模态重识别任务）和多模态行人重识别（可见光、红外、素描、文本联合输入的重识别任务）的基础概念、研究背景和各自的应用场景。思考以下问题：有监督和无监督行人重识别学习的基本流程是什么样的？跨模态和多模态任务的输入、学习目的有何不同？常用的特征学习损失函数有哪些，起到的作用是什么？了解组里常用的跨模态行人重识别框架。附行人重识别学习资源list1，2，相关文献：[23][24][25][26][27][28][31]
        2. 联邦学习：了解联邦学习的基础概念[1]，为什么有联邦学习这个需求。辨析联邦学习的类别、横向和纵向联邦学习。了解Global和Personal[2]联邦学习的区别和联系。了解不同联邦学习基础的训练流程，了解最基本的联邦学习baseline：FedAvg[3]，并且跑通一个demo，了解组里的通用联邦学习框架。思考以下问题：客户端本地如何更新？是否有中心服务器？服务器的作用是什么？客户端上传到服务器或者互相通信的内容（模型、特征等..）？附：联邦学习资源List，相关文献：[1][3][5][6][7][8][9][10][11][13][15]
        3. 多模态情感意图理解：了解多模态学习基本概念，是如何融合多模态特征，如何共同训练的[16][17][18][22]，推荐B站李沐老师的多模态相关的论文精读，多模态学习资源List。了解意图理解的概念[19]、多模态意图理解的概念和研究背景、现有方法[20][21][32][33][34][35]，思考以下问题：意图理解和传统图像分类任务的区别是什么？数据集构成有何特点？任务难点是什么？相关文献：[16][17][18][19][20][22][32][33][34][35]
        考核方式：面试提问。
    2. 第二章：文献阅读 (注意，为了避免形式主义的任务式阅读，有个小提醒：该部分的本质是为了提高你深入阅读论文的能力，提高你对方法抽象概念的理解，一些技术细节可以不用钻牛角尖，重点还是**论文背景和问题、论文动机和贡献解读**)。建议根据方向阅读以下给出的全部论文，并在所给文章中按个人喜好挑选5篇撰写阅读报告，建议选择较新的文献。
    1. 阅读报告的内容应包括：论文标题、作者单位、论文背景和问题、论文动机和贡献解读、方案设计详细分析、实验效果及其分析、结论、自己的思考等。撰写阅读报告时可以参考原论文、网络解读（博客、知乎专栏等）、跟进本论文的工作等。（注意：某些较为老旧的论文原文可能在Introduction和Method部分的写作较为晦涩，经常会出现难以理解的公式、符号等，此时不用灰心，完全可以依靠网络博客、视频等的解读来学习这部分的内容）这部分内容页数不限；注意，重点关注论文的背景、问题、动机、方法部分，实验部分适当取舍阅读，思考部分随意写一点自己真实的想法就可以（没必要上GPT来完成）
    考核方式：文献阅读报告。考核方向已在下方给出。你需要任意挑选一个方向，挑选其中的两篇论文阅读、完成文献阅读报告，并做一个PPT对这两篇论文进行展示。阅读和展示考核文献时，你尤其需要思考：本文的创新点在哪里？未来是否还有能继续研究的空间？你的思考是什么？
    重要提醒：在你看论文的时候，可能会看到一些复杂公式不好理解，容易望而生畏，这里有一些经验建议
        1. 跳过公式，理解算法核心，做概念抽象的理解
        2. 结合知乎或者优秀帖子看，先用中文和别人嚼碎的，会好接受一点
        3. 结合代码跑着（断点调试），理解抓住数据input和output，数据在整个model是如何流动的（forward）（关注matrix shape）
    3. 第三章：代码复现。你需要在选择的方向的中挑选任意两个文献（参考👆🏻给出的文献），根据他们给出的GitHub仓库跑通代码，提供跑通训练pipeline到test的完整流程，并对比其是否和原文中的结果吻合。建议选择最新的文献完成代码复现。
    考核方式：代码复现报告，附在文献阅读报告后，展示实验结果，并简单说明复现过程中遇到的问题和解决方案
    4. 第四章：未来展望（随意写一点真实感悟就好）。你需要在本文的全部内容的基础上，展现你在考核期间学习到的内容、所学内容的自我思考和未来展望，阐述自己在考核期间的心得和收获，并对考核任务的难度、平滑度进行点评和提出建议，同时阐明自己在研究生期间的学习目标和感兴趣的研究方向。
2. 文献调研报告的格式提示（尽量完成就好，也是规范日后学术PPT的标准）：
    1. 推荐一级标题四号字体，二级标题和正文部分都用小四；
    2. 中文一律使用微软雅黑，英文部分使用Times New Roman字体；
    3. 图片、表格需要有标题。涉及到参考文献的需要设置交叉引用；
    4. 可以使用Word，也可以使用LaTeX，以文档美观、易于阅读为最终目标。提交的报告文件必须是PDF格式；
    5. 报告应图文并茂、排版美观；代码部分建议截图，无需粘贴代码，以美观为重。
3. PPT格式提示：
    1. PPT可以用中文或英文制作；
    2. 不允许设置动画，导出为PDF格式；
    3. 中文一律使用微软雅黑，英文部分使用Times New Roman字体；
    4. 图片、表格需要有标题。涉及到参考文献的需要设置交叉引用；参考文献放在本页PPT的最下方；
    5. 可以使用Powerpoint，也可以使用LaTeX，以文档排版美观、易于阅读为最终目标。提交的报告文件必须是PDF格式。
如果对其他方向感兴趣，比如：持续学习、多模态医学人工智能（精神心理疾病、眼科）等，可以自行发挥，查询相关文献，也作为可选方向之一，下列培训内容具体文献未给出，可以根据兴趣自行选择，按对等要求完成，考核内容已给出。

行人重识别方向考核内容
Transformer for Object Re-Identification: A Survey. ArXiv 2024
Deep Learning for Person Re-identification: A Survey and Outlook. IEEE TPAMI 2024.
Channel Augmentation for Visible-Infrared Re-Identification
Cross-Modal Implicit Relation Reasoning and Aligning for Text-to-Image Person Retrieval
Towards Modality-Agnostic Person Re-identification with Descriptive Query
Augmented Dual-Contrastive Aggregation Learning for Unsupervised Visible-Infrared Person Re-Identification
All in One Framework for Multimodal Re-identification in the Wild
联邦学习方向考核内容
重点三篇Survey文章：
Heterogeneous Federated Learning: State-of-the-art and Research Challenges. ACM Computing Surveys 2023
Federated learning for generalization, robustness, fairness: A survey and benchmark. IEEE TPAMI 2024.
Vertical Federated Learning for Effectiveness, Security, Applicability: A Survey. arXiv 2024.
Rethinking Federated Learning With Domain Shift: A Prototype View
Robust Federated Learning With Noisy and Heterogeneous Clients
Federated Graph Learning under Domain Shift with Generalizable Prototypes
Fedproto: Federated prototype learning across heterogeneous clients.
FedPara: Low-Rank Hadamard Product for Communication-Efficient Federated Learning
多模态情感意图理解考核内容
Contextual Augmented Global Contrast for Multimodal Intent Recognition
Cross-Modality Pyramid Alignment for Visual Intention Understanding
Learnable Hierarchical Label Embedding and Grouping for Visual Intention Understanding
Emollm: Multimodal emotional understanding meets large language models
持续学习方向考核内容
Prototype augmentation and self-supervision for incremental learning
Self-Sustaining Representation Expansion for Non-Exemplar Class-Incremental Learning
Heterogeneous Continual Learning
Task-Aware Information Routing from Common Representation Space in Lifelong Learning
New insights on reducing abrupt representation change in online continual learning

医疗AI方向考核内容
Learning From Synthetic CT Images via Test-Time Training for Liver Tumor Segmentation. IEEE TMI 2022.
Concept-based Lesion Aware Transformer for Interpretable Retinal Disease Diagnosis. IEEE TMI 2024.
Cross-Feature Interactive Tabular Data Modeling With Multiplex Graph Neural Networks. IEEE TKDE 2024.
Automatic detection of 39 fundus diseases and conditions in retinal photographs using deep neural networks
IOP-FL: Inside-Outside Personalization for Federated Medical Image Segmentation                                                         

提交方式
文献阅读报告和文献汇报PPT完成后，导出为PDF格式提交到邮箱：yemang@whu.edu.cn，同时抄送到以下邮箱。之后，导师将在阅读文献报告后邀请学生做线下文献PPT汇报。
抄送邮件：
如果选择方向为"行人重识别或多模态情感意图理解方向": 抄送至yangbin_cv@whu.edu.cn
如果选择方向为"联邦学习": 抄送至wenkehuang@whu.edu.cn，guanchengwan@whu.edu.cn各一份。
如果选择方向为"持续学习": 抄送至wuxuanshi@whu.edu.cn
如果选择方向为"医疗AI": 抄送至lihe404@whu.edu.cn

Update: 24.1010 by Bin Yang
Update: 24.04 23 by wgc


参考文献
[1]: Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept and applications. 2019.
[2]: Yishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh. Three approaches for personalization with applications to federated learning. arXiv preprint arXiv:2002.10619, 2020.
[3]: Jakub Koneˇcn`y, H Brendan McMahan, Daniel Ramage, and Peter Richt´arik. Federated optimization: Distributed machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527, 2016.
[4]: Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, and Bingsheng He. A survey on federated learning systems: Vision, hype and reality for data privacy and protection. 2019.
[5]: Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2020.
[6]: Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Se[1]bastian U Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for on-device federated learning. In ICML, 2020.
[7]: Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fed{bn}: Federated learning on non-{iid} features via local batch normalization. In ICLR, 2021.
[8]: Qinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In CVPR, pages 10713–10722, 2021.
[9]: Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang, and Chengqi Zhang. Fedproto: Federated prototype learning across heterogeneous clients. In AAAI, 2022.
[10]: Othmane Marfoq, Giovanni Neglia, Richard Vidal, and Laetitia Kameni. Per[1]sonalized federated learning through local memorization. In ICML, pages 15070– 15092, 2022.
[11]: Wenke Huang, Mang Ye, and Bo Du. Learn from others and be yourself in heterogeneous federated learning. In CVPR, 2022.
[12]: Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2017.
[13]: Xingbo Fu, Binchi Zhang, Yushun Dong, Chen Chen, and Jundong Li. Federated graph machine learning: A survey of concepts, techniques, and applications. arXiv preprint arXiv:2207.11812, 2022.
[14]: Ke Zhang, Carl Yang, Xiaoxiao Li, Lichao Sun, and Siu Ming Yiu. Subgraph federated learning with missing neighbor generation. NeurIPS, 34:6671–6682, 2021.
[15]: Yue Tan, Yixin Liu, Guodong Long, Jing Jiang, Qinghua Lu, and Chengqi Zhang. Federated learning on non-iid graphs via structural knowledge sharing. In AAAI, 2023.
[16]: Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, Zicheng Liu, and Michael Zeng. An empirical study of training end-to-end vision-and-language transformers, 2021
[17]: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021.
[18]: Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, and Steven Hoi. Align before fuse: Vision and language representation learning with momentum distillation, 2021 
[19]: Intentonomy: a Dataset and Study towards Human Intent Understanding
[20]: MIntRec: A New Dataset for Multimodal Intent Recognition
[21]: MISA: Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis
[22]: BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation
[23]: Deep learning for person re-identification: A survey and outlook
[24]: CLIP-ReID: Exploiting Vision-Language Model for Image Re-Identiﬁcation without Concrete Text Labels
[25]: CMTR: Cross-modality Transformer for Visible-infrared Person Re-identiﬁcation
[26]: RGB-Infrared Cross-Modality Person Re-Identiﬁcation
[27]: Unsupervised Visible-Infrared Person Re-Identification via Progressive Graph Matching and Alternate Learning
[28]: Cross-Modal Implicit Relation Reasoning and Aligning for Text-to-Image Person Retrieval
[29]:  FedPara: Low-Rank Hadamard Product for Communication-Efficient Federated Learning
[30]:  Augmented Dual-Contrastive Aggregation Learning for Unsupervised Visible-Infrared Person Re-Identification
[31]:  SketchTrans: Disentangled Prototype Learning With Transformer for Sketch-Photo Recognition
[32]: Contextual Augmented Global Contrast for Multimodal Intent Recognition
[33]: Cross-Modality Pyramid Alignment for Visual Intention Understanding
[34]: Learnable Hierarchical Label Embedding and Grouping for Visual Intention Understanding
[35]: Emollm: Multimodal emotional understanding meets large language models

## Ref and Tag