---
id: fbbhwt128xaojy66nxffgv0
title: Getting-up_Policies_站立策略
desc: ''
updated: 1742190187685
created: 1742189717381
---

## 1. 作者团队信息
- 作者: Xialin He, Runpei Dong, Zixuan Chen, Saurabh Gupta
- 团队: 来自伊利诺伊大学厄巴纳-香槟分校（University of Illinois Urbana-Champaign）和西蒙弗雷泽大学（Simon Fraser University）

## 2. 背景和动机
- 背景: 人形机器人在实际应用中容易摔倒，且需要人类干预才能恢复。自动摔倒恢复是人形机器人可靠部署的关键前提。手动设计控制器来应对不同的摔倒姿势和复杂地形非常困难。
- 动机: 现有的基于模型的控制方法在应对新环境时泛化能力有限，而基于学习的方法在四足和双足机器人 locomotion 任务中取得了显著进展。然而，摔倒恢复任务涉及复杂的接触模式，需要准确建模碰撞几何和稀疏奖励，这使得直接应用现有方法效果不佳。

## 3. 相关研究
- 人形机器人控制: 包括基于零力矩点（ZMP）的模型控制、优化方法和模型预测控制（MPC）。近年来，基于学习的方法在 locomotion 任务中取得了显著进展。
- 腿部机器人摔倒恢复: 包括通过运动规划、手动设计的轨迹和基于强化学习（RL）的方法。现有方法在处理复杂地形和不可预测的初始状态时存在局限性。
- 角色动画中的摔倒恢复: 一些研究探索了基于 RL 的运动模仿算法，但这些方法通常使用简化动力学，难以直接迁移到真实机器人。

## 4. 核心思路
- 核心问题: 学习人形机器人在不同初始姿势和地形下的摔倒恢复策略。
- 解决方案: 提出一个两阶段的强化学习框架，第一阶段在简化条件下发现恢复动作，第二阶段在强控制正则化下将动作优化为可部署的策略。

## 5. 方案与技术
- 两阶段训练:
    - 第一阶段（发现策略）: 在简化条件下（弱正则化、固定初始姿势、简单碰撞几何）训练策略，发现恢复动作。
    - 第二阶段（可部署策略）: 在强控制正则化下（平滑性奖励、关节速度惩罚等）训练策略，跟踪第一阶段发现的动作轨迹，并进行地形和初始姿势的随机化。
- 策略架构: 使用多层感知机（MLP）作为策略模型，通过近端策略优化（PPO）进行训练。
- 奖励设计: 包括任务奖励（如高度奖励、对称性奖励）和正则化奖励（如平滑性奖励、关节速度惩罚）。

## 6. 实验与结论
- 实验设置: 在仿真和真实世界中使用 Unitree G1 人形机器人进行实验，测试了从仰卧和俯卧姿势恢复的能力。
- 实验结果: 提出的方法在仿真和真实世界中均表现出色，能够在不同地形和初始姿势下成功恢复。与基线方法相比，HumanUP 在任务成功率、动作平滑性和安全性方面表现更好。
- 结论: 两阶段训练方法有效解决了复杂接触模式下的摔倒恢复问题，成功实现了从仿真到真实世界的迁移。

## 7. 贡献
- 提出了一种两阶段的强化学习框架，用于学习人形机器人在不同初始姿势和地形下的摔倒恢复策略。
- 首次在真实世界中成功演示了 人形机器人从仰卧和俯卧姿势恢复的能力。
- 通过实验验证了 所提出方法在仿真和真实世界中的有效性和鲁棒性。

## 8. 不足
- 依赖高性能物理仿真平台，如 IsaacGym，当前机器人仿真器在接触动力学模拟的准确性和效率上仍有局限。
- 强化学习过程是一个欠定问题，难以确保学习到的动作完全符合人类行为，例如，学习到的动作可能会举起双手以保持平衡。

## Ref and Tag

[humanoid-getup](https://humanoid-getup.github.io/)