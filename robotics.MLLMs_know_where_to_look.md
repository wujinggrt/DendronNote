---
id: 88lxutih2k72osc4a5hfzlt
title: MLLMs_know_where_to_look
desc: ''
updated: 1752894976508
created: 1752370725596
---

## 论文总结

### 作者、团队信息、论文标题、论文链接、项目主页

*   **作者**: Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, Filip Ilievski
*   **团队信息**:
    *   University of Southern California, USA (南加州大学)
    *   Vrije Universiteit Amsterdam, The Netherlands (阿姆斯特丹自由大学)
*   **论文标题**: MLLMS KNOW WHERE TO LOOK: TRAINING-FREE PERCEPTION OF SMALL VISUAL DETAILS WITH MULTIMODAL LLMS (多模态大语言模型知道看哪里：用免训练的方式感知微小视觉细节)
*   **发表会议**: ICLR 2025
*   **论文链接**: [https://arxiv.org/abs/2502.17422](https://arxiv.org/abs/2502.17422)
*   **项目主页**: [https://github.com/saccharomycetes/mllms_know](https://github.com/saccharomycetes/mllms_know)

### 主要贡献

1.  **揭示并证实了 MLLM 的核心局限性**: 论文通过实验表明，多模态大语言模型 (MLLM) 在处理微小视觉细节方面表现不佳，并通过干预研究证实了视觉对象尺寸与模型性能之间存在 **因果关系**，而不只是相关性。
2.  **区分了感知与定位问题**: 论文通过分析模型内部的注意力机制，发现 MLLM 即使在给出错误答案时，也常常能准确地 **知道** 应该关注图像的哪个区域。这表明，问题根源在于 **感知能力不足** (看不清细节)，而非 **定位能力缺失** (找不到目标)。
3.  **提出了免训练的解决方案 (ViCrop)**: 基于上述发现，论文提出了一系列无需额外训练的视觉干预方法 (ViCrop)。这些方法利用 MLLM 自身的内部状态 (如注意力图和梯度图) 来自动定位并裁剪出图像中的关键区域，从而增强模型对小细节的感知。
4.  **验证了方法的有效性和通用性**: 在多个 MLLM 模型和多种视觉问答 (VQA) 基准数据集上的实验证明，ViCrop 方法能显著提升模型在处理细节敏感任务上的准确率，同时不会损害其在通用任务上的性能，展示了良好的效果和应用潜力。

### 研究背景（研究问题，研究难点和相关工作）

*   **研究问题**:
    随着 MLLM 在机器人、生物医学和自动驾驶等关键领域的广泛应用，理解并解决其感知能力的局限性变得至关重要。一个核心问题是：MLLM 能像处理大尺寸对象一样有效地感知微小的视觉细节吗？这种潜在的缺陷可能会给下游应用带来风险。

*   **研究难点**:
    1.  **因果关系不明**: 难以确定 MLLM 在小目标上的性能下降究竟是由于无法定位 (找不到)，还是无法感知 (找到了但看不清)。
    2.  **高昂的解决成本**: 传统上，提升模型对细节的感知能力通常需要更高分辨率的输入、更复杂的模型结构或大规模的重新训练，这些方法成本高昂且不易扩展。
    3.  **通用性与特异性的平衡**: 解决方案需要在提升细节感知能力的同时，不损害模型在处理需要全局信息的常规任务上的性能。

*   **相关工作**:

| 领域研究                    | 已有方法                                                                                            | 局限性                                                                       | 本文改进                                                                                                           |
| :-------------------------- | :-------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------- |
| **多模态大语言模型 (MLLM)** | 端到端预训练模型 (如 CLIP)；模块化预训练模型 (如 BLIP-2, LLaVA)。                                   | 预训练成本极高；模型内部的感知缺陷未被充分研究。                             | 提出一种 **免训练** 的增强方法，可与现有模型架构正交结合，作为补充。                                               |
| **视觉定位方法**            | 专用模型 (如 YOLO, SAM)；原生方法 (如 Grad-CAM)；工具使用 (Tool-using)；思维链 (Chain-of-thought)。 | 专用模型依赖大量标注；原生方法泛化性有限；工具使用和思维链增加了流程复杂性。 | 证明 MLLM 已具备内在定位能力，并提出一种更通用、直接利用该内在能力的方法，无需外部工具或复杂推理。                 |
| **MLLM 的视觉感知局限性**   | 高分辨率微调；多智能体流水线 (Multi-agent pipelines)。                                              | 微调成本高，且无法扩展至任意分辨率；多智能体系统复杂且难以部署。             | 深入分析并证实了问题的根源是 **感知而非定位**，并提出了一种轻量级、可扩展、免训练的 **视觉裁剪 (cropping)** 方案。 |

### 方法

本文的核心方法是 **自动视觉裁剪 (Automatic Visual Cropping, ViCrop)**。其基本思想是：既然模型知道看哪里，就帮它“放大看”。具体流程如下：

1.  **识别兴趣区域**: 首先，利用 MLLM 处理原始图像和问题的内部信息，生成一个“重要性图”，标示出与问题最相关的图像区域。本文提出了三种生成重要性图的方法：
    *   **相对注意力 ViCrop (rel-att)**: 计算模型对特定问题的注意力与对一个通用问题（如“描述这张图片”）的注意力的比值，从而得到归一化的 **相对注意力图** $$ A_{rel} $$。这可以有效过滤掉与问题无关的背景噪声。
        $$ A_{rel}(x, q) = \frac{A_{si}(x,q)}{A_{si}(x,q')} $$
        其中 $ A_{si} $ 是答案到图像的注意力，$ x, q $ 分别是图像和问题，$ q' $ 是通用指令。
    *   **梯度加权注意力 ViCrop (grad-att)**: 利用模型决策对注意力分数的梯度来为注意力图加权。梯度的大小反映了该注意力对最终决策的敏感度，因此可以突出与语义最相关的部分。
        $$ \bar{A}_{si}(x, q) = \bar{A}_{st}(x, q) \otimes \bar{A}_{ti}(x, q) $$
    *   **输入梯度 ViCrop (pure-grad)**: 直接计算模型决策对输入图像的梯度 $$ G(x,q) = ||\nabla_x v(x, q)||_2 $$，并结合高通滤波等图像处理技术来强化边缘信息，生成重要性图。

2.  **选择与裁剪边界框**: 在生成的重要性图上，使用多尺度的滑动窗口来寻找包含最多“重要性”信息的最佳边界框 (bounding box)。

3.  **整合裁剪后的信息**: 将裁剪出的图像区域 (zoom-in view) 重新缩放到模型的标准输入尺寸，然后将其 token 序列与原始图像的 token 序列 **拼接** 在一起，共同作为 MLLM 的输入。这样，模型既能看到全局信息，又能聚焦于关键细节。

4.  **高分辨率图像策略**: 对于超高分辨率的图像 (如 V* 数据集)，采用两阶段策略：先将大图切块，分别计算重要性图并拼接，然后再进行边界框选择和裁剪，以避免在初始缩放时丢失过多细节。

第三节，指出 VLMs 对尺寸敏感。更大的图像更容易得到预期回答。这样来说，腕部相机提供查看更大图像的能力，提供的信息更加丰富。当然，远离操作物体时，视野里的信息明显少于头部相机。所以需要关注和设计。

第四节，VLMs 尽管答错，但也知道往哪儿看。为了评估往哪儿看的能力，作者使用量化的方法研究，分析 Transformer layers 的注意力图。量化关于整张图像的 spatial attention，比较 GT 的 BB 中注意力总数与其他同等大小 BB 区域的注意力分数总数。

分析 xformer 生成的 token 对于图像 token 的注意力（softmax 得到的注意力分数），研究图像 token 对于生成内容的影响。关于所有 layers，从生成的 tokens 开始，提取它们关于图像 tokens 的 softmax cross-attention (softmax 的每个头得到 seq_len x seq_len 的注意力矩阵，分别对应每个 q 对应各个内容的注意力)，可以得 $A_{st}(x,q)\in \mathbb{R}^{L \times H \times 1 \times T}$，L 是层数，此维度对应每层, H 对应各个注意力头，T 对应图像 tokens 数量。随后，对于所有头求平均。

再分析每个图像区域对于图像 token 的重要性。基于 xformer 的 connector 中，提取每层中，每个 image token 关于 ViT 输出 tokens 的 softmax cross-attention。LLaVA 系列则不做此分析，仅得到单位矩阵。

回顾 VLMs 的处理方式，ViT 先拆分为 patches，经过 connector 映射到 LLMs 的输入空间，加入到问题 tokens 之前。如此思路，方便对齐和微调。是否动作专家也需要对齐呢？此外，对齐之后是否可以舍弃其余部分？比如，只用 Decoder 生成一个 token。利用对齐后的内容作为编码器。就像 MAE 工作的思路。


### 实验与结论

*   **实验设置**:
    *   **模型**: InstructBLIP (Vicuna-7B), LLaVA-1.5 (Vicuna-7B)
    *   **数据集**:
        *   细节敏感型: TextVQA, V*, POPE, DocVQA
        *   通用型: GQA, AOKVQA, VQAv2
*   **核心发现**:
    1.  **尺寸敏感性**: 表 1 显示，所有 MLLM 的准确率都随着视觉目标尺寸的减小而显著下降。而使用人工提供的真实边界框进行裁剪 (human-CROP) 后，准确率大幅提升，这有力地证明了尺寸对性能的 **因果性** 影响。
    2.  **定位准确性**: 图 3 的注意力比率 (attention ratio) 分析显示，在绝大多数网络层中，模型对正确答案区域的关注度远高于随机区域 (比率大于 1)，即使在它回答错误的情况下也是如此。这表明 MLLM **知道看哪里**。
    3.  **ViCrop 有效性**: 表 2 的结果显示，三种 ViCrop 方法在所有细节敏感的数据集上都显著提升了模型的准确率 (例如，LLaVA-1.5 在 TextVQA 上的准确率从 47.80% 提升至 56.06%)，同时在通用数据集上保持了原有性能，甚至略有提升。
*   **结论**:
    MLLM 对微小视觉细节的感知偏差是一个普遍存在的关键限制。该限制的根源是 **感知能力** 而非定位能力。本文提出的 ViCrop 是一套可扩展、免训练的有效解决方案，它通过利用模型自身的内部知识来指导视觉裁剪，为提升 MLLM 在细节敏感场景下的可靠性提供了一个充满前景的方向。

### 不足

1.  **全局信息受限**: ViCrop 通过聚焦单个区域来增强细节感知，因此对于需要理解多个对象之间关系或进行计数的复杂问题，其效果有限。
2.  **计算开销**: 尽管合理，但 ViCrop 仍然引入了额外的推理时间开销（在 GPU 上约 1-2 秒）。
3.  **未来方向**: 论文提出，未来的工作可以探索将 ViCrop 扩展到同时关注多个区域，通过量化等方法优化其推理成本，以及探索如何结合不同 ViCrop 方法的优势。


## Ref and Tag