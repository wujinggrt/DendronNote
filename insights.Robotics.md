---
id: eszoks0gixjd0fjyul4wh10
title: Robotics
desc: ''
updated: 1741365048744
created: 1740293600917
---

## HiRT, Helix, DexVLA
[[robotics.HiRT_使用分层机器人Transformer提示机器人控制]]，[[robotics.Helix：用于通才人形机器人控制的_VLM]]，[[robotics.DexVLA]]

HiRT 和 Helix 解耦了机器人使用大模型和策略模型，在处理时延时，使用异步策略。把 VLM 当作大型的视觉-语言编码器，动作策略用 token 当作条件来训练。如果一次编码生成多个 token，是否意味着是规划的多个动作，需要序列执行？或者是并行执行？多个 token，可能是双臂协作，四臂协作。

想法：在视频方面，特别是腕部，使用一张图像，不断放大，模拟机械臂接近物体。随后可以用这些放到某个物体的图像制作伪视频，类比机械臂接近物体，用于强化学习。借鉴 [[rl.LongShortTermImagination]]。


### 关注 VLM 推理和规划的能力

DexVLA 本质还是 VLM 预测并调用技能库。重要的还是让模型具有更强大的规划能力。DexVLA 提出使用 sub-step 标注的方法，提高长范围规划能力。

提供规划能力，是推理模型？使用微调，还是强化学习，还是修改网络结构？资源有限，探究小模型的可能。

多模态中，图片是一个模态，语言是一个模态，可以查看 Qwen2.5-VL 如何增强此推理能力，还有关于 Math/Coding 的推理能力增强工作。

是否需要一个动作的编码器？

## 动作指令需要简洁，规划需要有序
可以规定，动作指令不应该超过多少文本 (比如 20 个 tokens)，以细粒度来保证精简。训练 policy 时可以把此文本作为 condition 加入起来。

此文本可以有两个选择：(1) 是直接给 policy 嵌入一个可学习的嵌入层；(2) 是直接用 VLM 编码的 token 作为潜在表示来学习。

规划层，则把动作指令分解为一系列动作，包含一个停止动作。

探索式地，自增长技能，补充技能库。我想，这是把动作指令编码为 token，才有可能实现。但是，平滑工作十分重要，离散的情况，不能够有效和稳定地学习。基于技能库，不断探索，修改。这部分如何证实正在做，如何观测，是一个难点。思路：token 比文本可能蕴含更懂信息，压缩得更准确。那么，需要保证两个模型，policy 模型和 VLM 都能一直理解，则要求 VLM 能够解译 token，代表理解，比如 prompt 解释这个 token 对应什么动作，需要完成什么目的，即 What and How，应该再来个 why，为什么能成功，是基于什么推理，Why 则对模型提出了推理能力的要求。而 policy 就像小脑，和脊柱神经，快速执行。

## VLA 解释 token，policy 理解和执行 token
需要微调 (全量 or LoRA) VLM 来解释 token。每个 token 就像加密的暗号，需要 VLM 和 policy 能够解译，就像古代的虎符，对应权力。比如，在图中看到了 XXX， Xxx 是目标，token 要求，需要怎样操作它。以前的 VLM 只关心输出，而不关心解释，这个 idea 引入了自解释。自解释使用另一个 reward model，增强潜向量生成能力。

以 VLA 为出发点去探索。

最后会只需要 Decoder 吗？当数据足够大，是否可以参考语音工作 ([Step-Audio](https://github.com/stepfun-ai/Step-Audio)) 的对齐。

DexVLA 附录的消融实验指出，对扩散专家的预训练是十分重要的。所以对 policy 要预训练。扩散专家引入了文本，可以引入 token 作为 Condition，但是粒度更小。粒度越小，policy 的动作 horizon 越小，那么要求 VLM 规划的范围越大 (即包含更多的 step，潜 token 更多）。把规划输出文字变为规划输出 token，可以使得控制动作的粒度更小，这在动作上是一个 tradeoff，因为动作难以对应多个文本，文本表示内容太多了，应当压缩为 token 来给 policy 提供条件。token 要可解释，起到精简的效果。token 就像神经递质。

安全和可靠性，验证 token 是否正确生成；token 是否正确执行。


Motivation，重新审视人如何从语言，比如书籍记载，学习技能。通过阅读，思考，实践和尝试，得到反馈，对比和思考和总结，再实践。如此迭代。再思考人体决策组件是如何作用，大脑是核心，具有十分多的神经细胞，学习记忆期间，不同区域的细胞会有突触，从不连接变得连接。大脑不能直接分析语言和获取语言，只能由各个器官，类似传感器，接受“多模态”数据，然后再分析。分析时，大脑只是神经细胞间不断传递信号。而我们思考的内容，也就是在找关联，神经突触也是在找关联和建立连接。说明激活的神经网络层，是有希望的。LLM 只是处理 token。也许 few-shot 和推广能力，是基于足够强大的推理能力的模型上实现的。few-shot 关键在于捕捉关键信息，合理规划动作。实际上，还是大脑传递神经递质给各个器官，然后由他们执行。那么，参数数量，推理能力和想象能力，核心还是大脑，也许核心还是 LLM。各个模态，需要配合它，对齐起来。语言到时候只是一个解释性的模态了。

神经递质，想办法从不连接变换为连接。另一个点，在于动作的执行，协调程度不是由大脑控制，由小脑控制。小脑自然不会理解语言，只知道电信号，神经递质。这里可以看一下，是电信号还是神经递质。

具体所做，关注 fusion，在于高效捕捉信息关键部分。LLM 输出 latent tokens，各个模块学习和**对齐**。再这个能力之上，再探索式的学习，即灵活运用。

DexVLA 指出，使用子步骤推理至关重要。每个任务拆分和标记多个子步骤。那么，更细粒度的子步骤，是否更好，更方便探索。

### Warmup 是和 VLM 结合的启动范式

就像 R1 需要有 V3 来 warmup，类似鸡生蛋蛋生鸡。DexVLA 附录指出，先预训练扩散专家，是必要的，VLM 要一开始能够看得见有成功的例子，才能慢慢纠正。如果没有预训练扩散专家的 warmup，全是失败的例子，自然难以学习如何理解。

### 也许遗忘是泛化的关键

遗忘后，通过学习几个 demo，然后再快速分析 substeps，相当于 warmup。这个过程可以放到执行动作之前，“大脑”预演一遍。

遗忘，可以遗忘坏的习惯，拟合到特定场景的局限性。是泛化能力的关键。

通才应该是善于遗忘，但是能够马上 warmup。

### 短时记忆，cache 与空间感知

人眼在转过头之后，刚才看到的物体其实脑袋还有些许印象，几秒钟，记忆还很清楚。几分钟，还行。半小时，有一些残留记忆。但是久了，比如几个小时之后，便记不清。这和 cahce 置换策略类似。

把遗忘想成置换策略，把短时记忆，想成 cache。刚才看过的内容，那么应该还在记忆里。那么，Agent 如何实现这个短时记忆，是个关键。cache 如何设计？是内存？张量？还是buffer？这是难点一。

第二个关键，是空间感知。通过记忆来实现空间感知，还需要知道移动的相对性。短时的移动，说明不会造成失忆。但是长时间的移动，感兴趣区域的 attention 便被刷新了。可以尝试一个较小记忆的 agent，验证想法。机械臂短时的移动，比如 UMI 中，暂时离开目标区域，但是要能记住刚才场景，要记得住空间位置，记得住原来桌子在哪儿，桌上物体相对目前机械臂位置是多少。可以不是绝对坐标，但是要有适应性，Adaptive 的。

可以想象，机械臂移动，腕部相机捕获的照片也随之移动。但是，Agent 要能够“想象”，刚才位置在哪儿，要能够规划找到回去的移动轨迹。可以是适应性地，不用那么精准，只用大致的，能接受一定误差的。

比如，通过照片序列，推理刚才所在位置。就像实现 MAE 一样，移动机械臂，根据看到图像，就像在 cache 上绘制“清明上河图”。视线离开的地方，即 MAE 中遮掩了的内容，但是可以靠想象补全出来。并且，知道方位还可以移动到那儿，然后再根据视觉抓取。也许，思路类似及时建图，SLAM，但是处理的是 RGB，泛化性更好，更容易交互。

我认为，这是解决长范围任务的关键所在。比如，我记得刚才那儿有一堆东西。这样方便人形机器人移动。那么，使用一个机械臂，有一个相机，也可以模拟。

这样，超声的问题，似乎也能解决搜索。

## Ref and Tag