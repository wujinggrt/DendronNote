---
id: eszoks0gixjd0fjyul4wh10
title: Robotics
desc: ''
updated: 1747246277235
created: 1740293600917
---

## HiRT, Helix, DexVLA
[[robotics.HiRT_使用分层机器人Transformer提示机器人控制]]，[[robotics.Helix：用于通才人形机器人控制的_VLM]]，[[robotics.DexVLA]]

HiRT 和 Helix 解耦了机器人使用大模型和策略模型，在处理时延时，使用异步策略。把 VLM 当作大型的视觉-语言编码器，动作策略用 token 当作条件来训练。如果一次编码生成多个 token，是否意味着是规划的多个动作，需要序列执行？或者是并行执行？多个 token，可能是双臂协作，四臂协作。

想法：在视频方面，特别是腕部，使用一张图像，不断放大，模拟机械臂接近物体。随后可以用这些放到某个物体的图像制作伪视频，类比机械臂接近物体，用于强化学习。借鉴 [[rl.LongShortTermImagination]]。


### 关注 VLM 推理和规划的能力

DexVLA 本质还是 VLM 预测并调用技能库。重要的还是让模型具有更强大的规划能力。DexVLA 提出使用 sub-step 标注的方法，提高长范围规划能力。

提供规划能力，是推理模型？使用微调，还是强化学习，还是修改网络结构？资源有限，探究小模型的可能。

多模态中，图片是一个模态，语言是一个模态，可以查看 Qwen2.5-VL 如何增强此推理能力，还有关于 Math/Coding 的推理能力增强工作。

是否需要一个动作的编码器？

## 动作指令需要简洁，规划需要有序
可以规定，动作指令不应该超过多少文本 (比如 20 个 tokens)，以细粒度来保证精简。训练 policy 时可以把此文本作为 condition 加入起来。

此文本可以有两个选择：(1) 是直接给 policy 嵌入一个可学习的嵌入层；(2) 是直接用 VLM 编码的 token 作为潜在表示来学习。

规划层，则把动作指令分解为一系列动作，包含一个停止动作。

探索式地，自增长技能，补充技能库。我想，这是把动作指令编码为 token，才有可能实现。但是，平滑工作十分重要，离散的情况，不能够有效和稳定地学习。基于技能库，不断探索，修改。这部分如何证实正在做，如何观测，是一个难点。思路：token 比文本可能蕴含更懂信息，压缩得更准确。那么，需要保证两个模型，policy 模型和 VLM 都能一直理解，则要求 VLM 能够解译 token，代表理解，比如 prompt 解释这个 token 对应什么动作，需要完成什么目的，即 What and How，应该再来个 why，为什么能成功，是基于什么推理，Why 则对模型提出了推理能力的要求。而 policy 就像小脑，和脊柱神经，快速执行。

## VLA 解释 token，policy 理解和执行 token
需要微调 (全量 or LoRA) VLM 来解释 token。每个 token 就像加密的暗号，需要 VLM 和 policy 能够解译，就像古代的虎符，对应权力。比如，在图中看到了 XXX， Xxx 是目标，token 要求，需要怎样操作它。以前的 VLM 只关心输出，而不关心解释，这个 idea 引入了自解释。自解释使用另一个 reward model，增强潜向量生成能力。

以 VLA 为出发点去探索。

最后会只需要 Decoder 吗？当数据足够大，是否可以参考语音工作 ([Step-Audio](https://github.com/stepfun-ai/Step-Audio)) 的对齐。

DexVLA 附录的消融实验指出，对扩散专家的预训练是十分重要的。所以对 policy 要预训练。扩散专家引入了文本，可以引入 token 作为 Condition，但是粒度更小。粒度越小，policy 的动作 horizon 越小，那么要求 VLM 规划的范围越大 (即包含更多的 step，潜 token 更多）。把规划输出文字变为规划输出 token，可以使得控制动作的粒度更小，这在动作上是一个 tradeoff，因为动作难以对应多个文本，文本表示内容太多了，应当压缩为 token 来给 policy 提供条件。token 要可解释，起到精简的效果。token 就像神经递质。

安全和可靠性，验证 token 是否正确生成；token 是否正确执行。


Motivation，重新审视人如何从语言，比如书籍记载，学习技能。通过阅读，思考，实践和尝试，得到反馈，对比和思考和总结，再实践。如此迭代。再思考人体决策组件是如何作用，大脑是核心，具有十分多的神经细胞，学习记忆期间，不同区域的细胞会有突触，从不连接变得连接。大脑不能直接分析语言和获取语言，只能由各个器官，类似传感器，接受“多模态”数据，然后再分析。分析时，大脑只是神经细胞间不断传递信号。而我们思考的内容，也就是在找关联，神经突触也是在找关联和建立连接。说明激活的神经网络层，是有希望的。LLM 只是处理 token。也许 few-shot 和推广能力，是基于足够强大的推理能力的模型上实现的。few-shot 关键在于捕捉关键信息，合理规划动作。实际上，还是大脑传递神经递质给各个器官，然后由他们执行。那么，参数数量，推理能力和想象能力，核心还是大脑，也许核心还是 LLM。各个模态，需要配合它，对齐起来。语言到时候只是一个解释性的模态了。

神经递质，想办法从不连接变换为连接。另一个点，在于动作的执行，协调程度不是由大脑控制，由小脑控制。小脑自然不会理解语言，只知道电信号，神经递质。这里可以看一下，是电信号还是神经递质。

具体所做，关注 fusion，在于高效捕捉信息关键部分。LLM 输出 latent tokens，各个模块学习和**对齐**。再这个能力之上，再探索式的学习，即灵活运用。

DexVLA 指出，使用子步骤推理至关重要。每个任务拆分和标记多个子步骤。那么，更细粒度的子步骤，是否更好，更方便探索。

### Warmup 是和 VLM 结合的启动范式

就像 R1 需要有 V3 来 warmup，类似鸡生蛋蛋生鸡。DexVLA 附录指出，先预训练扩散专家，是必要的，VLM 要一开始能够看得见有成功的例子，才能慢慢纠正。如果没有预训练扩散专家的 warmup，全是失败的例子，自然难以学习如何理解。

### 也许遗忘是泛化的关键

遗忘后，通过学习几个 demo，然后再快速分析 substeps，相当于 warmup。这个过程可以放到执行动作之前，“大脑”预演一遍。

遗忘，可以遗忘坏的习惯，拟合到特定场景的局限性。是泛化能力的关键。

通才应该是善于遗忘，但是能够马上 warmup。

### 仅仅 latent 可能不够

需要掩码的方案，比如 DexGraspVLA 才能行得通。VLM 生成 hidden tokens，再加上遮掩。但是 ViT 又展示了，这个 latent 变量，提取的特征，cls_token，能够学到很多内容。因为是可学习，与其他内容一起送入 Transformer，能提取更多内容。

### 短时记忆，cache 与空间感知

人眼在转过头之后，刚才看到的物体其实脑袋还有些许印象，几秒钟，记忆还很清楚。几分钟，还行。半小时，有一些残留记忆。但是久了，比如几个小时之后，便记不清。这和 cahce 置换策略类似。

把遗忘想成置换策略，把短时记忆，想成 cache。刚才看过的内容，那么应该还在记忆里。那么，Agent 如何实现这个短时记忆，是个关键。cache 如何设计？是内存？张量？还是buffer？这是难点一。

第二个关键，是空间感知。通过记忆来实现空间感知，还需要知道移动的相对性。短时的移动，说明不会造成失忆。但是长时间的移动，感兴趣区域的 attention 便被刷新了。可以尝试一个较小记忆的 agent，验证想法。机械臂短时的移动，比如 UMI 中，暂时离开目标区域，但是要能记住刚才场景，要记得住空间位置，记得住原来桌子在哪儿，桌上物体相对目前机械臂位置是多少。可以不是绝对坐标，但是要有适应性，Adaptive 的。

可以想象，机械臂移动，腕部相机捕获的照片也随之移动。但是，Agent 要能够“想象”，刚才位置在哪儿，要能够规划找到回去的移动轨迹。可以是适应性地，不用那么精准，只用大致的，能接受一定误差的。

比如，通过照片序列，推理刚才所在位置。就像实现 MAE 一样，移动机械臂，根据看到图像，就像在 cache 上绘制“清明上河图”。视线离开的地方，即 MAE 中遮掩了的内容，但是可以靠想象补全出来。并且，知道方位还可以移动到那儿，然后再根据视觉抓取。也许，思路类似及时建图，SLAM，但是处理的是 RGB，泛化性更好，更容易交互。

我认为，这是解决长范围任务的关键所在。比如，我记得刚才那儿有一堆东西。这样方便人形机器人移动。那么，使用一个机械臂，有一个相机，也可以模拟。

这样，超声的问题，似乎也能解决搜索。

是否使用 GRU 实现 cache？

### 快慢记忆

慢记忆需要足够多，快记忆来得快去得快，但是时间局部性表现好。

### Satial，空间智能：想象和记忆是关键

相机即 head，使用上下左右的方位感知，加上中间，即九个方位。得到空间感知。

生成，也想与“想象”有关，是值得考虑的。借助 long-short imagination 的工作，借助其思路。短期记忆确实应该用 LSTM，或者 gru 吗？还是 Attention？感觉还是用 DiT，token 加语言描述，“我刚才到过XX地方”，“记得场景式XX的”，描述特征的 token，或者是 latent token，随后可以送给 DiT，用于想象，以生成想象内容。此外，还要记住方位？各个方位可能还有什么物体。这可能需要视频，图像序列来处理。

几秒前（时间短，算力要求低）的想象原场景，分辨出操作的物体。想象，可以想象空间，而非建图和存储。暂时存储某些noise？还是 xond？想象可以说损失信息的，可以生成较为模糊，类似马赛克的场景，但只要有大致信息就好。想象和记忆，甚至可以是单通道的。根据多个想象，dit 想象，llm 推理向哪一个方向去查找。llm 规划和决定查找，进一步移动机械臂，获取更清晰的图像，才进行操作。这是空间智能吗？训练 dit，生成低质的图像。当前图像，加上方向作为条件，预测低质图像，即回忆和想象。在根据想象，还可以预测想象前后左右的图像，这是链式的，可以不断预测远方空间，但越远越“记不清”。这是一种链式的思想。训练时，可以这样想象，看到图像后，移动左上方，预测其右下方作为想象，作为 label。

应用，只需要一张地图，二维的即可，后面可以让 agent  自动适应这个空间。通过这样的自监督训练，根据 latent feature，保存在 cache，后续可以拿来“想象”，预测刚才到过，或者看过的空间。但是，这基于在提供图像的场景，能够较好抓取的情况。编码，还有 latent 变量，可以蕴含关键信息。更多细节信息，使用相机，或其他传感器来提供。这样足够处理泛化的场景了。

场景：可以先在导航，验证召回情况。在切换到操作场景，找物品场景。

Mask is attention？在场景，使用 mask，高效注意？

上下文注意力，还是自回归的 decoder 比较在行，关于记忆。

### DexGraspVLA

mask 作为视觉的注意力关键，通过视觉，通过 mask 找到操作的物体。进一步地，可以看做 VLM 找到 affordance，随后提取 mask 给低级策略，由它操作。DexGraspVLA 看重物体，那么，是否可以改进为关心 affordance，把标边框变为标 affordance 部分。

DexGraspVLA 只有抓取的能力，仅仅训练了抓取。如果，我想要 VLM 规划了握手，挥手，触碰开关，那么需要如何实现？这才是泛化性。挥手时，便没有了视觉重心，不需要 mask。握手则需要。思考解决思路：利用泛化性，VLM 上下文包含挥手定义，可以询问 VLM 什么是挥手和打招呼。随后规划一系列动作。为了能够规划，动作需要更细粒度。参考 DexVLA 等工作，最好将细粒度的动作规划为 1-5s 内。比如，"Close it"，"Grasp it" 等。那么，还是需要使用 latent variable 才能够处理双系统的通信和对齐，这样才有强化学习的可能。

#### 细粒度动作

应当包含哪些？结合平时操作，手臂应当包含靠近、远离、发力（快速地靠近），手指则包含抓、按。

### 推理能力

推理能力都是基于足够强大的基座模型。比如 V3 和 Qwen。

### 需要解耦到多少程度？

人眼顶住一个目标，能够跟随它移动，似乎不是大脑思考的，而是激活某种**状态**，盯紧的状态来实现的。就像是发出了指令：接下来，你需要聚焦这个物体。如果是这种方式，激活某种功能，用 Sam2+Cutie 的组合足够，不涉及 VLM。

VLM 主要是发出指令。那么，类似 HiRobot 的方案，把小命令作为激活状态的指令，发送给底层策略，似乎能够更自然地衔接 VLM。如果需要更细致的掌控，那么微调 VLM 来学习如何发出这种激活低级策略模型的 token。

VLM 微调，就像人一样，通过视觉，学习别人如何操作的，一步步关注别人的动作，比如如何操作某个工具。可以通过视觉，总结短命令，学习到为了完成某个大任务，规划出短命令。先靠近，再抓取。还是只用挥手。

再思考学习。需要想出一种方法，从视觉（来源于网络视频、现实演示），收集到数据。然后标定（不能够从一点都不会的时候开始，强思考能力模型也是从强基座模型 RL 训练的，所以预训练小模型很有用，代表预先会一些技能，然后再进一步探索，参考 DexVLA）。学习短命令，分析短命令如何组合，完成大任务（这就类似数学能力了，这样便把机器人的规划问题转化为了与学习数学能力相似的问题）。VLM 的思考能力，可以参考数学和昆仑万维的模型，还有视觉思考的模型。

RL 不会带来新知识，所以有些内容，还是要先通过其他方式学会（预训练的语料可以对标模仿学习的新技能，比如挥手、抓取、按开关）。

短命令即 token，类似的工作，还有 sentence 即 token，可以参考。

如何操作，关键还在于定位 affordance，这需要思考，需要 VLM 完成，可能用视觉推理模型来做。

双系统还是很有必要。解耦是需要必要的。思考的部分在于分析和规划，思考来源于分析和预测一系列短命令执行后，是否能够完成任务，从而规划。底层策略则是“肌肉记忆”，比如靠近，还有跟踪物体（属于感知部分），快速执行。思考和执行是异步的。

### 不拆分大小模型的弊端

复杂度，训练难度上涨，难以追溯原因和训练。

### insights

创新点：
1. 不同试图，相机的图像，在不同阶段的注意力应当不同。腕部较远时用头部相机，腕部近时关注腕部。使用门控网络，以此确定注意力机制。或者直接用 mask 来处理掉头部的注意力。可以找找 CVPR 是否有多图像对比的工作。比如字节跳动的 Seed1.5-VL 能够找不同，可以启发。再比如英伟达的双目深度估计，FoundationStereo 工作，也许一双优秀的双眼是好的选择？
2. VLM 来规划抓取方向。机械臂移动前，首先确定从上方抓取合适，还是从侧面抓取合适，可以和人类对齐（通过反馈，思考等方式，从而提高可解释性）。可以选择小的 VLM，比如 MiniMind-V，因为暂时不需要其他知识面，对于实验室资源来说，是一个折中的选择，也好训练。抓取规划可以参考 ManiTrans 的工作。
3. 理解深度的 VLA，特别是 VLM 部分。训练一个小的 VLM，重点在于理解深度。比如，MiniMind-V，使用 FoundationStereo 作为视觉编码器。编辑文本，构造相关深度的数据集。还需要识别物体的数据集。关键在于识别物体，还有深度。训练 VLM 的 3D Gounding 理解，可以使用 Qwen2.5 VL 或者字节的 Seed1.5-VL 打框，制作常见场景图像的数据集。训练方式可以参考千问和字节的 Seed1.5-VL 如何增加 Gounding and Counting，3D Spatial Understanding 能力。

深度部分，使用类似 FoundationStereo 的模型，关于图片，使用一个 Depth Anything V2 估计单目深度；再使用一个共享参数的 CNN 网络，来提取共同的视觉特征。再使用一个 Context 网络模块提取上下文信息。

可以在比赛上，查看引入深度信息到扩散策略的表现。

## VRFT + Robot

VLM 进行 VRFT，使用 Reinforce++ 或 GRPO。随后，小模型在使用 CLIP-text encoder 和 DINOv2 提供视觉。

VLM 需要有识别能力。随后，能够识别，才能够思考。正如 DeepSeek R1 的工作。思考如何与机器人工作配合。

### 视觉理解和规划的大小模型

视觉也可大小模型理解，然后规划？就像人眼可以快速扫动，盯住 (contact) 一个地方才能思考。

VLM 做分析 (analyse) 和指导 (instruct)。指导小模型找到感兴趣区域，通过短命令 + CLIP-Text encoder。参考 ORION: A Holistic End-to-End Autonomous Driving Framework by Vision-Language Instructed Action Generation

## 杂乱场景

解决杂乱场景是否更有效？每个相机都需要加边框和 mask 吗？这样杂乱也能 focus。

## Agent 设计

细粒度的 Agent 设计：如果机器人需要完成一个动作，比如抓取，可以设计为 GraspAgent。如果是挥手和递东西，那么是 PassAgent。这样，再设计一个调度的 PlanningAgent 来 dispatch 任务。

粗粒度的 Agent 设计：只有一个 RobotAgent，首先继承自 PlanningAgent，把动作规划为技能调用，即 ToolCalls，向 VLM 进行 ask。比如，清理桌面，清理一半，拿起一个物体时，告诉它那不是垃圾，你得放下。

## MCP 启发

是否大小模型的方式，也可以规范一套协议？

Tool 的描述会提供给 LLM，生成合理的规划。是否机器人也需要提供一套描述，比如动作的描述，动作的参数，动作的条件，动作的限制，形成一套协议。

## In-Context Edit

[[diffusion.In-Context_Edit_只需微调很小的参数可以编辑图像]]

启发：加入 subtask 的文字描述，靠近的动作，是不是也可以如此地嵌入？

应当思考让扩散专家更好的匹配 VLM，类似 IoC 的思想，这样才能对扩散专家扩展，尽可能把大模型当做上层抽象，减少修改和调整。对能力模块扩展，对决策模块的修改关闭。

靠近的效果不太好，是否可以用文字提示，还没抓取便用 Close it 提示向前抓取，从而避免过早回来。这些文字就是 In-Context Edit Prompt。达到提升控制扩散生成内容的效果。

或者是，训练好了之后，通过 Edit 来提高泛化能力。

### 扩散的奖励模型

关于 Early Filter Inference Time Scaling 部分，如何训练一个 Critic Model，或者 Reward Model 来给生成的动作打分？这对强化学习很关键。

是否可以使用 VLM 判断生成动作是否完好？但是机器人领域不像文生图，生成轨迹并没有直观的图像，要执行后才能看到结果。用 VLM 判断两张图像是否正在接近抓取目标？以此设计奖励函数。或者使用 DPO。

使用拟合的打分方式。使用余弦相似度，在训练时，生成的动作越类似 label，那就得分越高。

可以参考：

DPO 助力Diffusion更好的生图、修图 - 走遍山水路的文章 - 知乎
https://zhuanlan.zhihu.com/p/13036626546
> Diffusion Model Alignment Using Direct Preference Optimization

Diffusion Reward

## 能否创建一个 Context？

Context 包含了很多 VLM 和扩散专家共同理解的特征。直接用 VLM 的 ViT？

## Ref and Tag