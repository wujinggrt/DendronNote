---
id: xpu09lebljhjk44y07t7alc
title: GR3
desc: ''
updated: 1753605987776
created: 1753551380171
---

Generated markdown
## 论文总结

### 作者、团队信息、论文标题、论文链接、项目主页

*   **作者**: ByteDance Seed 团队. [1]
*   **团队信息**: ByteDance Seed. [1]
*   **论文标题**: GR-3 Technical Report. [1]
*   **论文链接**: arXiv:2507.15493v2 [cs.RO] 22 Jul 2025. [1]
*   **项目主页**: https://seed.bytedance.com/GR3. [1]

### 主要贡献

该论文介绍了 GR-3，一个大规模的视觉-语言-动作 (vision-language-action, VLA) 模型，其主要贡献如下：

1.  **强大的泛化能力**: GR-3 能够泛化到新颖的物体、环境以及包含抽象概念的指令. [1]
2.  **高效的数据适应性**: 模型能够通过少量的人类轨迹数据进行高效微调，从而实现对新场景的快速、低成本适应. [1, 10]
3.  **卓越的长时程与灵巧任务性能**: GR-3 在需要双手协作和移动操作的长时程、高灵巧性任务中表现出高鲁棒性和可靠性. [1, 2]
4.  **创新的训练方法**: 提出了一种多层面的训练方案，该方案结合了网络规模的视觉语言数据协同训练、通过 VR 设备收集的人类轨迹数据进行的高效微调，以及基于机器人轨迹数据的有效模仿学习. [1]
5.  **定制化的机器人平台**: 推出了专为 GR-3 设计的多功能双臂移动机器人 ByteMini，该机器人具有出色的灵活性和可靠性. [1]

### 研究背景（研究问题，研究难点和相关工作）

*   **研究问题**: 研发能够辅助人类完成日常任务的通用智能机器人，是机器人领域一个长期的目标. [1] 这类机器人需要具备强大的泛化能力，能够处理现实世界中各种各样的场景和任务. [1]

*   **研究难点**:
    1.  **泛化性**: 现实世界的多样性要求机器人策略具有极强的泛化能力，以应对新颖的场景. [1]
    2.  **长时程与复杂操作**: 许多日常任务本质上是长时程的，并需要复杂的灵巧操作，这对机器人策略的鲁棒性和可靠性提出了很高的要求. [1]
    3.  **数据效率**: 传统的 VLA 模型通常需要大量的演示数据进行训练，这使得向新环境的适应变得困难且成本高昂. [1]
    4.  **指令理解**: 对于分布之外 (out-of-distribution) 的指令，特别是那些涉及新颖物体类别或需要复杂推理的概念的指令，仍然是一个重大挑战. [1]

*   **相关工作**:

| 领域研究                                                                        | 已有方法                                                                                                                                | 局限性                                                                                               | 本文改进                                                                                                                                              |
| :------------------------------------------------------------------------------ | :-------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------- |
| **通用操作策略 (Generalist Manipulation Policies)**                             | 训练跨不同机器人形态的策略 (cross-embodiment) [13]；利用预训练的视觉语言模型 (VLM) 开发机器人策略 [13]；从网络视频数据中学习 [13]。     | 收集真实世界的机器人轨迹数据成本高昂且耗时. [13]                                                     | 引入了 GR-3，这是一个在机器人轨迹和视觉语言数据上联合训练的 VLA 模型，并且可以用少量人类轨迹数据进行高效微调. [13]                                    |
| **机器人操作的多模态协同训练 (Multi-Modal Co-Training for Robot Manipulation)** | 从预训练的视觉编码器或 VLM 初始化策略 [13]；在机器人轨迹和视觉语言数据上共同微调大型 VLM (如 RT-2) [14]。                               | 早期的方法主要依赖于大规模的机器人轨迹数据，或者在泛化到需要抽象概念理解的复杂指令方面能力有限. [14] | GR-3 采用类似的协同训练策略，但精心设计了一个包含多种视觉语言任务的网络规模数据集，从而在零样本泛化到未见过的物体和复杂指令方面展示了强大的能力. [14] |
| **利用人类数据进行策略训练 (Leveraging Human Data for Policy Training)**        | 从人类视频中提取不同类型的表征以增强策略学习 [14]；利用 VR 设备和手部追踪技术，通过少量机器人数据与人类视频共同训练来提升策略性能. [14] | 如何有效地将从人类演示中学习到的知识迁移到机器人上，尤其是在数据稀疏的情况下，仍然是一个挑战. [10]   | GR-3 展示了其能够通过极少量的人类轨迹数据（例如每个物体 10 条轨迹）进行有效微调，从而高效地适应新颖的设置. [10]                                       |

### 方法

GR-3 是一个端到端的视觉-语言-动作 (VLA) 模型，它通过生成一个长度为 $k$ 的动作块 $a_t = a_{t:t+k}$ 来控制一个双臂移动机器人. [4]

1.  **模型架构**:
    *   GR-3 采用了 mixture-of-transformers 架构. [4]
    *   它使用一个预训练的视觉语言模型 (VLM)，即 **Qwen2.5-VL-3B-Instruct**，来处理来自多个摄像头视图的图像观测和自然语言指令. [4]
    *   动作预测则通过一个  Action Diffusion Transformer, DiT 完成. [4]
    *   为了稳定训练过程并提升语言跟随能力，模型在 DiT 模块的注意力和前馈网络中额外应用了 **RMSNorm**. [4]
    *   整个 GR-3 模型包含 40 亿个参数. [4]

2.  **训练方案**:
    GR-3 的训练结合了三种不同来源的数据，使其能够泛化、高效适应并稳健地执行任务. [4]
    *   **机器人轨迹数据的模仿学习**:
        *   模型通过最大化专家演示数据 $D$ 上的策略对数似然来进行训练，具体使用流匹配 (flow-matching) 损失函数进行监督. [4]
        *   损失函数定义为：$$L_{action}(\theta) = \mathbb{E}_{\{a_t, o_t, s_t, l\} \sim D} [\| v_{\theta}(l, o_t, s_t, a_t^{\tau}) - u(a_t^{\tau} | a_t) \|^2]$$. [4]
        *   为了解决多视角带来的虚假相关性问题，引入了“任务状态”（进行中、已终止、无效）作为额外的动作维度进行辅助监督. [5]

    *   **视觉语言数据的协同训练 (Co-Training)**:
        *   为了赋予模型处理分布外 (OOD) 指令的能力，GR-3 将机器人轨迹数据与大规模的视觉语言数据联合训练. [5]
        *   机器人数据用于训练 VLM 主干和 DiT 模块（使用流匹配目标），而视觉语言数据仅用于训练 VLM 主干（使用下一词元预测目标）. [5]
        *   这种协同训练使得模型能够理解新颖的语义概念，如尺寸、空间关系和常识. [5]

    *   **人类轨迹数据的少样本泛化 (Few-Shot Generalization)**:
        *   为了实现快速和低成本的适应，GR-3 可以在少量通过 VR 设备 (PICO 4 Ultra Enterprise) 收集的人类轨迹数据上进行微调. [5]
        *   人类轨迹数据的收集效率远高于机器人遥操作（约 450 条/小时 vs 250 条/小时）. [5]

3.  **硬件系统**:
    *   **ByteMini 机器人**: 一个 22 自由度 (DoF) 的双臂移动机器人，专为灵巧操作、高可靠性和用户友好性而设计. [6] 其臂部采用球形腕关节设计，实现了类人的灵活性. [6]

### 实验与结论

该论文通过三个具有挑战性的真实世界任务对 GR-3 进行了广泛评估：**通用取放**、**长时程桌面清理** 和 **灵巧的衣物悬挂**. [8]
*   **通用取放 (Generalizable Pick-and-Place)**:
    *   **设置**: 在四种不同设置下进行评估：基础、未见过的环境、未见过的指令、未见过的物体. [9]
    *   **结果**:
        *   在所有设置中，GR-3 的指令跟随率和成功率均优于基线模型 πο. [9]
        *   尤其是在“未见过的指令”和“未见过的物体”这两个最具挑战性的设置中，GR-3 表现出显著的优势，成功率分别从 40% 提升至 77.1% 和 57.8%. [10] 这证明了与视觉语言数据协同训练的有效性. [10]
        *   通过少样本学习，仅用每个未见物体 10 条人类轨迹进行微调，成功率可从 57.8% 进一步提升至 86.7%. [10]
*   **长时程桌面清理 (Long-Horizon Table Bussing)**:
    *   **设置**: 在“扁平设置”（一个通用指令完成整个任务）和“指令跟随设置”（按顺序执行多个子任务指令）下评估模型的鲁棒性. [10]
    *   **结果**: GR-3 在两种设置下均优于 πο，特别是在指令跟随设置中，成功率从 53.8% 大幅提升至 97.5%. [12] GR-3 能够严格遵循指令，泛化到新的物体和目的地，并能拒绝执行无效任务. [12]
*   **灵巧的衣物悬挂 (Dexterous Cloth Manipulation)**:
    *   **设置**: 在三种设置下进行评估：基础、位置变化、未见过的衣物. [12]
    *   **结果**: GR-3 在所有设置中均优于 πο，在基础和位置变化设置中分别取得了 86.7% 和 83.9% 的平均任务进展. [13] 即使面对袖子长度不同的未见过的衣物，GR-3 也能实现 75.8% 的平均任务进展，展现了其处理灵巧任务和泛化到新颖实例的能力. [13]
*   **结论**:
    实验结果全面展示了 GR-3 在理解涉及抽象概念的复杂指令、有效泛化到新颖物体和环境、从极少的人类演示中高效学习，以及以卓越的鲁棒性和可靠性执行长时程和灵巧任务方面的出色能力. [14] GR-3 的发展为构建能够辅助人类日常生活的通用机器人迈出了重要一步. [14]

### 不足

尽管 GR-3 在具有挑战性的任务中表现出色，但仍存在一些局限性：

1.  **指令理解错误**: 在处理涉及新颖概念和物体的未见过指令时，GR-3 仍然会犯错. [14]
2.  **抓取未知物体**: 对于形状未见过的物体，模型在抓取时会遇到困难. [14]
3.  **模仿学习的局限**: 与所有模仿学习方法类似，GR-3 在 rollout 过程中可能会陷入分布外的状态并且无法从中恢复. [14]

未来的工作计划包括扩大模型和训练数据规模，并引入强化学习 (RL) 来增强模型在复杂任务中的鲁棒性，并优化其性能，以突破模仿学习的局限. [14]


## QA

### 为什么用 flow matching，对比扩散优势在哪儿？

核心优势在于 训练更稳定、推理速度更快，这对于需要实时或近实时响应的机器人控制任务至关重要。

## Ref and Tag