---
id: 2l5nw6bxqnhp42w12n7vw8d
title: TODO
desc: ''
updated: 1740906981771
created: 1738165674608
---

## LLM

Triton写算子入门 - Arthur的文章 - 知乎
https://zhuanlan.zhihu.com/p/887257776

rStar 小模型推理
https://arxiv.org/abs/2501.04519
https://github.com/microsoft/rStar
OpenAI 的 O1 模型的原理是什么？ - 猛猿的回答 - 知乎
https://www.zhihu.com/question/666999747/answer/4472268952

自主机器人将强化学习与基础模型相结合：方法与观点 - 黄浴的文章 - 知乎
https://zhuanlan.zhihu.com/p/20365147329

RL 框架：verl，https://arxiv.org/pdf/2409.19256v2

通过 Affordance 链改进视觉-语言-动作模型 - 黄浴的文章 - 知乎
https://zhuanlan.zhihu.com/p/21713958996

R1-V 突破 2K Star，持续进化中！ - Lei Li的文章 - 知乎
https://zhuanlan.zhihu.com/p/22989750949
https://github.com/Deep-Agent/R1-V

多模态R1复现之旅… - pinkman的文章 - 知乎
https://zhuanlan.zhihu.com/p/22890208704

大模型强化学习面经 - 一蓑烟雨的文章 - 知乎
https://zhuanlan.zhihu.com/p/659551066

人型机器人行走
https://why618188.github.io/beamdojo/

多智能体协作综述
https://arxiv.org/abs/2501.06322

SMAC-R1：在MARL中复现R1时刻 - 赵鉴的文章 - 知乎，星际争霸
https://zhuanlan.zhihu.com/p/24922558098

笔记：MoBA 与 Native Sparse Attention - 刀刀宁的文章 - 知乎
https://zhuanlan.zhihu.com/p/24774848974


DeepSpeed 的 ZeRO 解读。

摸着Logic-RL，复现7B - R1  zero - aaaaammmmm的文章 - 知乎
https://zhuanlan.zhihu.com/p/25982514066

在 Qwen2.5-VL 复现 R1
https://github.com/om-ai-lab/VLM-R1

【论文解读】LLM-Microscope：揭秘 LLM 中不起眼 Token 的隐藏力量 - tomsheep的文章 - 知乎
https://zhuanlan.zhihu.com/p/26492642537

UCB cs294/194-196 Large Language Model Agents 课程笔记 - Perf的文章 - 知乎
- https://zhuanlan.zhihu.com/p/26269945565
- [CS294/194-196 Large Language Model Agents](https://rdi.berkeley.edu/llm-agents/f24)
- [UCB_CS294_LLmAgents](https://github.com/WangYuHang-cmd/UCB_CS294_LLmAgents)

## Robotics

MapNav 使用了 GPT 标注图像数据。可以借鉴处理。
https://arxiv.org/pdf/2502.13451

https://github.com/Peterwangsicheng/RoboBERT

RoboGrasp：一种用于稳健机器人控制的通用抓取策略 - 黄浴的文章 - 知乎
https://zhuanlan.zhihu.com/p/22946605267

独家专访｜清华TEA实验室负责人：具身智能入门/转行到底学什么？ - 深蓝学院的文章 - 知乎
https://zhuanlan.zhihu.com/p/26333134789

VLA 等各种工作合集。
博士想读具身智能/智能机器人应该怎么规划自己的科研？ - EyeSight1019的回答 - 知乎
https://www.zhihu.com/question/655570660/answer/87040917575

LLamaV-o1: (https://mbzuai-oryx.github.io/LlamaV-o1/)

## RL
在线强化学习改进VLA模型 - 黄浴的文章 - 知乎
https://zhuanlan.zhihu.com/p/23993973779

面向长范围交互式 LLM 智体的强化学习 - 黄浴的文章 - 知乎
https://zhuanlan.zhihu.com/p/24109661682

TRL 
https://huggingface.co/docs/trl/main/en/index

GPRO, R1
https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/mini-deepseek-r1-aha-grpo.ipynb

导航 RL
https://github.com/Zhefan-Xu/NavRL

[大模型 30] SWE-RL 强化学习提高模型软件工程能力 - hzwer 黄哲威的文章 - 知乎
https://zhuanlan.zhihu.com/p/26792881958

九坤联合微软亚洲研究院等成功复现 DeepSeek-R1，具体水平如何？ - 薛定谔的猫的回答 - 知乎
https://www.zhihu.com/question/13238901947/answer/111931939432

九坤联合微软亚洲研究院等成功复现 DeepSeek-R1，具体水平如何？ - 到处挖坑蒋玉成的回答 - 知乎
https://www.zhihu.com/question/13238901947/answer/112109118245
> 项目中的奖励函数设计对其他类似任务有重要的实践意义，建议 RL 做其他任务的学习下。如果 reward 判定写得不够严密，模型学习过程容易钻空子，骗取高 reward。K & K 是合成逻辑谜题 (K & K puzzle) 数据集。
> 参考：https://github.com/Unakar/Logic-RL/blob/main/verl/utils/reward_score/kk.py

## PyTorch 技巧
一文学会 Pytorch 中的 einsum - 梁德澎的文章 - 知乎
https://zhuanlan.zhihu.com/p/361209187

## Hack 和工程能力

长远看算法岗真的比开发岗香吗？ - 要没电了的回答 - 知乎
https://www.zhihu.com/question/409815271/answer/87375346326

抽象，简化和领域驱动设计 - 阿莱克西斯的文章 - 知乎
https://zhuanlan.zhihu.com/p/77026267