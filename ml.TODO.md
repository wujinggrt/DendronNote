---
id: 2l5nw6bxqnhp42w12n7vw8d
title: TODO
desc: ''
updated: 1744268486071
created: 1738165674608
---

## LLM Reasoning

rStar 小模型推理
https://arxiv.org/abs/2501.04519
https://github.com/microsoft/rStar

OpenAI 的 O1 模型的原理是什么？ - 猛猿的回答 - 知乎
https://www.zhihu.com/question/666999747/answer/4472268952

自主机器人将强化学习与基础模型相结合：方法与观点 - 黄浴的文章 - 知乎
https://zhuanlan.zhihu.com/p/20365147329

RL 框架：verl，https://arxiv.org/pdf/2409.19256v2

R1-V 突破 2K Star，持续进化中！ - Lei Li的文章 - 知乎
https://zhuanlan.zhihu.com/p/22989750949
https://github.com/Deep-Agent/R1-V

多模态R1复现之旅… - pinkman的文章 - 知乎
https://zhuanlan.zhihu.com/p/22890208704

大模型强化学习面经 - 一蓑烟雨的文章 - 知乎
https://zhuanlan.zhihu.com/p/659551066

人型机器人行走
https://why618188.github.io/beamdojo/

多智能体协作综述
https://arxiv.org/abs/2501.06322

SMAC-R1：在MARL中复现R1时刻 - 赵鉴的文章 - 知乎，星际争霸
https://zhuanlan.zhihu.com/p/24922558098

笔记：MoBA 与 Native Sparse Attention - 刀刀宁的文章 - 知乎
https://zhuanlan.zhihu.com/p/24774848974

DeepSpeed 的 ZeRO 解读。

摸着Logic-RL，复现7B - R1  zero - aaaaammmmm的文章 - 知乎
https://zhuanlan.zhihu.com/p/25982514066

在 Qwen2.5-VL 复现 R1
https://github.com/om-ai-lab/VLM-R1

【论文解读】LLM-Microscope：揭秘 LLM 中不起眼 Token 的隐藏力量 - tomsheep的文章 - 知乎
https://zhuanlan.zhihu.com/p/26492642537

UCB cs294/194-196 Large Language Model Agents 课程笔记 - Perf的文章 - 知乎
- https://zhuanlan.zhihu.com/p/26269945565
- [CS294/194-196 Large Language Model Agents](https://rdi.berkeley.edu/llm-agents/f24)
- [UCB_CS294_LLmAgents](https://github.com/WangYuHang-cmd/UCB_CS294_LLmAgents)

LLamaV-o1: (https://mbzuai-oryx.github.io/LlamaV-o1/)

昆仑万维正式开源了全球首个工业界多模态推理模型 Skywork R1V，你对该模型有哪些评价？ - tomsheep的回答 - 知乎
https://www.zhihu.com/question/15333687654/answer/128260535572
> 使用 MLP 简单缝合了 ViT 和 LLM，简单的理念可以学习。

## Robotics

MapNav 使用了 GPT 标注图像数据。可以借鉴处理。
https://arxiv.org/pdf/2502.13451

https://github.com/Peterwangsicheng/RoboBERT

RoboGrasp：一种用于稳健机器人控制的通用抓取策略 - 黄浴的文章 - 知乎
https://zhuanlan.zhihu.com/p/22946605267

独家专访｜清华TEA实验室负责人：具身智能入门/转行到底学什么？ - 深蓝学院的文章 - 知乎
https://zhuanlan.zhihu.com/p/26333134789

## VLA

通过 Affordance 链改进视觉-语言-动作模型 - 黄浴的文章 - 知乎
https://zhuanlan.zhihu.com/p/21713958996

VLA 等各种工作合集。博士想读具身智能/智能机器人应该怎么规划自己的科研？ - EyeSight1019的回答 - 知乎
https://www.zhihu.com/question/655570660/answer/87040917575

基于RoboTwin生成海量数据Finetune RDT-1B等具身大模型保姆级教程 - 穆尧的文章 - 知乎
https://zhuanlan.zhihu.com/p/22754193110

具身智能VLA方向模型fine-tune（单臂）（24.12.26已完结）
https://blog.csdn.net/iamjackjin/article/details/144534904?fromshare=blogdetail&sharetype=blogdetail&sharerId=144534904&sharerefer=PC&sharesource=qq_39422041&sharefrom=from_link
> 作者采集了 150 条数据，每个任务 10-30 条，

https://wildlma.github.io/
> 移动操作机器人

## RL
在线强化学习改进VLA模型 - 黄浴的文章 - 知乎
https://zhuanlan.zhihu.com/p/23993973779

面向长范围交互式 LLM 智体的强化学习 - 黄浴的文章 - 知乎
https://zhuanlan.zhihu.com/p/24109661682

TRL 
https://huggingface.co/docs/trl/main/en/index

GPRO, R1
https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/mini-deepseek-r1-aha-grpo.ipynb

导航 RL
https://github.com/Zhefan-Xu/NavRL

[大模型 30] SWE-RL 强化学习提高模型软件工程能力 - hzwer 黄哲威的文章 - 知乎
https://zhuanlan.zhihu.com/p/26792881958

九坤联合微软亚洲研究院等成功复现 DeepSeek-R1，具体水平如何？ - 薛定谔的猫的回答 - 知乎
https://www.zhihu.com/question/13238901947/answer/111931939432

九坤联合微软亚洲研究院等成功复现 DeepSeek-R1，具体水平如何？ - 到处挖坑蒋玉成的回答 - 知乎
https://www.zhihu.com/question/13238901947/answer/112109118245
> 项目中的奖励函数设计对其他类似任务有重要的实践意义，建议 RL 做其他任务的学习下。如果 reward 判定写得不够严密，模型学习过程容易钻空子，骗取高 reward。K & K 是合成逻辑谜题 (K & K puzzle) 数据集。
> 参考：https://github.com/Unakar/Logic-RL/blob/main/verl/utils/reward_score/kk.py

Logic-RL: Deepseek R1复现中的七大发现! 用益智谜题强化学习竟能提升数学竞赛水平！ - 涮月亮的谪仙人的文章 - 知乎
https://zhuanlan.zhihu.com/p/25355823769
> reinforce++ 性价比最高

https://hijkzzz.notion.site/reinforce-plus-plus

Logic-RL 表现不错，值得学习。

LLM RL入门 - Reku的文章 - 知乎
https://zhuanlan.zhihu.com/p/27172237359

R1 小模型复刻失败经验总结
https://mp.weixin.qq.com/s?__biz=Mzg5MTU1NTE1OQ==&mid=2247492943&idx=1&sn=aed1d150faaebb17ac0b9c3d3347e436&chksm=ce0e65e1751e208b29ad2c6c024d4f54504f1d5c43939e447f805d02b447de619f30fd1fd1a3&mpshare=1&scene=23&srcid=03115atkpiBzet6MMBOt0cSX&sharer_shareinfo=f1c7bb00c78885ad976322928466b740&sharer_shareinfo_first=f1c7bb00c78885ad976322928466b740#rd

用极小模型复现R1思维链的失败感悟 - 林正的文章 - 知乎
https://zhuanlan.zhihu.com/p/27699656438
> rl无法带来新知识

【论文解读】Chain Of Draft：LLM 少写多想更高效 - tomsheep的文章 - 知乎
https://zhuanlan.zhihu.com/p/28074420230

Diffusion Model 和强化学习，稀疏奖励。
https://arxiv.org/pdf/2503.11240

复现 DeepSeek-R1 小记 - 周星星的文章 - 知乎
https://zhuanlan.zhihu.com/p/32485048223

Open-Reasoner-Zero：基于基础模型的强化学习扩展开源方法 - 无影寺的文章 - 知乎
https://zhuanlan.zhihu.com/p/28504120030
> 数据更重要，不要过分追求复杂的方法，而要关注基础要素的规模效应。真正重要的是训练数据、模型规模和训练次数的规模，而不是设计的复杂度

https://github.com/zwq2018/embodied_reasoner

https://arxiv.org/abs/2503.21696

大模型Reward Model做不上去怎么办必读论文 - xhchen的文章 - 知乎
https://zhuanlan.zhihu.com/p/14681365893
> reward model 的论文

### World Model

【强化学习教程 20】世界模型（World Models） - tomsheep的文章 - 知乎
https://zhuanlan.zhihu.com/p/21498615281
> 使用策略预测环境的变化

【论文解读】IRIS：用Transformer构建世界模型 - tomsheep的文章 - 知乎
https://zhuanlan.zhihu.com/p/21353183728

## Agent

Manus发布一天后迅速出现OpenManus、OWL 等复刻项目，怎么做到的？ - 锦恢的回答 - 知乎
https://www.zhihu.com/question/14321968965/answer/119732180420
> 学习 OpenManus，做成熟和工程化的 Agent。

李宏毅：从零开始搞懂 AI Agent - tomsheep的文章 - 知乎
https://zhuanlan.zhihu.com/p/29123783155

【论文解读】：START：让大模型学会「借工具思考」 - tomsheep的文章 - 知乎
https://zhuanlan.zhihu.com/p/28933816497

## Hack 和工程能力

长远看算法岗真的比开发岗香吗？ - 要没电了的回答 - 知乎
https://www.zhihu.com/question/409815271/answer/87375346326

抽象，简化和领域驱动设计 - 阿莱克西斯的文章 - 知乎
https://zhuanlan.zhihu.com/p/77026267

## 部署
有没有vLLM/SGLang多机多卡部署详细教程？ - DefTruth的回答 - 知乎
https://www.zhihu.com/question/1888903744047011293/answer/1888905234241258191

## 灵巧手和 affordance

ManiTrans 和 AffordDP

## Cpp 和开发

> 各种项目，比如 dperf。LevelDB 和 RocksDB，网络编程 libuv，协议处理 kcp，异步编程 Asio（先看完 p2444r0）和 c++26 的 Sender/Receiver，数值计算 Eigen，模板看微软的 proxy，日志库 nanolog 和 spdlog，Actor 模式和 thread per core 思想参考 seastar (质量极高)，Json parser 看 simdjson，lock free 看 xenium (slab 思想可以优化 nanolog)，编译器编译参考 fmtlib
你读过的最好的 C++ 开源代码是什么？ - George的回答 - 知乎
https://www.zhihu.com/question/21376384/answer/50289239106

## 简历制作

突出自我学习能力。从以下几个点展开：

### 增强专业能力

任务完成的过程中，速度快，质量高。

解决问题，独立想办法设计方案并解决。不能解决的，能够准确问到点子上，定位能力强。

学习新知识能力强，擅长总结规律，一点就通，举一反三。

### 增强工作能力

态度端正，主动积极。空闲时候，主动学习和融入业务，持续学习专业知识，杜绝躺平摆烂心态。

及时反馈问题，不逃避，主动寻求解决办法。主动找人学习公司的开发工具，熟系开发流程和业务逻辑等知识。

善于总结反思。在问题上，要总结经验吸取教训，不能出现再犯的情况。

### Resume

参考开源大佬 [ice1000](https://ice1000.org/resume.html)。他的实习经历十分丰富，比如：

**JetBrains Research**, RemoteJan, 2020 – Dec, 2020
HoTT and Dependent Types, Interactive Theorem Prover Development
- Used features like gradle composite build and buildSrc to reduce build time and improve automation.  - Improved the language/IDE, such as sections, hygiene macros, Fin type with elaborative subtyping, semantic highlighting, etc.
- Created an extensible REPL engine, provided implementations in CLI (with contextual completion using jline3) and in IntelliJ IDEA (interacts with the opened project, supports completion, highlighting and goto definition).
- Designed and implemented an expression type-checking debugger that supports step-into and displays local context and expressions as stack frames.

**Sourcebrella Inc.**, Shenzhen, ChinaFeb, 2018 – Jul, 2018
Static Analysis, Compiler Frontend, IDE Plugin Development
- Created IntelliJ/CLion/Eclipse plugin for the Pinpoint analyzer. Co-worked on the SonarQube plugin.
- Created a multi-threading cross Java/Kotlin source code indexer which can index Hadoop within 4 minutes.
- Learned a lot about Linux programming and the Clang/LLVM codebase.

实习经历中，突出二到四个有亮点的点。体现使用什么工具做了什么，或是从中学到什么。突出了自己会的技能。