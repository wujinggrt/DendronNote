---
id: hnwwk4gfpy2umuc8naqu6lo
title: 动作分块流策略的实时执行
desc: ''
updated: 1751166592174
created: 1751125151950
---

## 论文总结

### 作者、团队信息、论文标题、论文链接、项目主页

*   **论文标题**: Real-Time Execution of Action Chunking Flow Policies (动作分块流策略的实时执行)
*   **作者**: Kevin Black, Manuel Y. Galliker, Sergey Levine
*   **团队信息**: Physical Intelligence, UC Berkeley
*   **论文链接**: [pdf](https://www.pi.website/download/real_time_chunking.pdf)
*   **项目主页**: [https://pi.website/research/real_time_chunking](https://pi.website/research/real_time_chunking)

### 主要贡献

1.  **提出实时分块 (RTC) 算法**: 提出了一种新颖的、仅在推理时使用的算法 RTC，它能够对任何基于扩散或流模型的 VLA (视觉-语言-动作模型) 进行平滑的异步执行，无需重新训练。该方法将异步执行问题建模为一个修复 (inpainting) 问题，以确保动作块之间的连续性。
2.  **构建新的仿真基准**: 鉴于现有基准大多是准静态的，作者设计了一个包含 12 个高度动态任务的新基准 (Kinetix 模拟器)，用于测试和验证算法在动态环境下的性能。
3.  **验证了真实世界的性能**: 在 6 个具有挑战性的真实世界双臂操作任务上 (包括移动操作)，验证了 RTC 的有效性。实验表明，RTC 速度快、性能好，并且对推理延迟具有独特的鲁棒性，显著提高了任务吞吐量和在精确任务（如点燃火柴）上的成功率。

### 研究背景

#### 研究问题
现代机器人，特别是通用机器人，越来越多地采用大型的视觉-语言-动作模型 (VLA)。这些模型虽然能力强大，但推理延迟非常高。在机器人控制中，一种常用的技术是动作分块 (Action Chunking)，即模型一次性预测未来一小段时间 (一个 chunk) 的动作序列。然而，高延迟导致在执行完一个动作块后，机器人**必须停下来等待下一个块的生成，造成明显的停顿**。如果采用异步执行（即在执行当前块的同时生成下一个块），又会在块的**边界处产生不连续、不协调的“抽搐”动作**，因为新生成的块没有考虑当前正在执行的动作，导致状态分布偏移。

#### 研究难点
1.  **高延迟与实时性矛盾**: 大型 VLA 模型动辄数十亿参数，推理耗时长，与机器人需要高频 (如 50Hz) 实时控制的需求相矛盾。
2.  **块间连续性**: 异步执行时，如何确保新生成的动作块能够与前一个未执行完的动作块平滑地衔接，避免产生高加速度、不自然的抖动。
3.  **鲁棒性**: 算法需要对网络波动、计算资源变化等原因造成的推理延迟变化具有鲁棒性。
4.  **通用性**: 解决方案最好是推理时算法，无需修改模型或复杂的重训练流程，以便能“开箱即用”地应用于现有的各种 VLA 模型。

#### 相关工作对比

| 领域研究           | 已有方法                                                                    | 局限性                                                                                 | 本文改进                                                                             |
| :----------------- | :-------------------------------------------------------------------------- | :------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------- |
| **动作分块与 VLA** | 使用扩散模型、流匹配、VQ-VAE 等生成模型来学习动作块。                       | 未解决高延迟导致的执行停顿或异步执行时的不连续问题。                                   | 提出 RTC 算法，通过“修复”技术在推理时平滑地连接异步生成的动作块。                    |
| **降低推理延迟**   | 模型蒸馏 (Consistency Policy)、流式扩散 (Streaming Diffusion)、并行解码等。 | 只能加速推理，但无法将单次前向传播的延迟降至零。当延迟仍高于控制周期时，问题依旧存在。 | 不试图消除延迟，而是管理延迟。通过异步执行和修复，使系统能够容忍显著的延迟。         |
| **实时执行**       | 模型预测控制 (MPC)、时延强化学习、分层 VLA 架构。                           | MPC 依赖显式动力学模型；RL 方法不一定适用于模仿学习；分层架构需要重新设计训练流程。    | 纯推理时算法，无需模型或训练改动，直接适用于现有的模仿学习 VLA。                     |
| **修复与引导**     | 图像修复 (Image Inpainting)、基于扩散的规划 (Diffuser)。                    | 主要用于图像生成或长时程规划，未被应用于解决机器人控制中的实时异步执行问题。           | 创新性地将“修复”思想应用于实时控制，将已执行的动作作为“已知区域”来引导生成后续动作。 |
| **双向解码 (BID)** | 通过拒绝采样来保持块间连续性，实现闭环控制。                                | 计算成本高（需要采样多个批次），且未明确考虑推理延迟带来的影响。                       | RTC 在计算上更高效，且在仿真中证明了在有延迟的情况下性能优于 BID。                   |

### 方法

本文的核心思想是将异步动作分块问题建模为一个 **修复 (Inpainting)** 问题。其关键在于，当开始生成一个新的动作块时，机器人已经在执行上一个块了。等到新块生成完毕时（经过 `d` 个时间步的延迟），上一个块的前 `d` 个动作已经被执行了。因此，新块的生成必须与这个“既成事实”相符。

RTC 算法分为以下几个关键步骤：

1.  **“冻结”与“修复”**: 推理新动作需要多少时间步，则跳过多少时间步。
    *   在时刻 `j-d` 开始为时刻 `j` 生成新的动作块。
    *   此时，我们已经知道从 `j-d` 到 `j-1` 时刻，机器人将会执行旧动作块中的相应动作。
    *   因此，在新块的生成过程中，我们将新块中对应这 `d` 个时间步的动作“冻结” (freeze)，使其与旧块中即将执行的动作保持一致。
    *   然后，模型的目标就变成了“修复” (inpaint) 新块中剩余的部分，使其与这个被冻结的前缀自然、平滑地衔接。

2.  **基于引导的流匹配 (Guidance-based Flow Matching)**:
    *   该方法建立在一种名为 $\Pi \text{GDM}$ 的免训练修复算法之上。在流匹配的每个去噪步骤中，向学习到的速度场 `v` 添加一个引导项。
    *   这个引导项会计算当前生成的噪声动作 `A_τ` 与目标（即旧块的动作前缀 `Y`）之间的差异，并生成一个梯度，将生成过程“推向”与目标一致的方向。
    *   修正后的速度场更新公式为：
        $$ v_{\text{IGDM}}(A_\tau^t, o_t, \tau) = v(A_\tau^t, o_t, \tau) + \min\left(\beta, \frac{1-r_\tau}{r_\tau}\right) (Y - \hat{A}_\tau^1) \text{diag}(W) \frac{\partial \hat{A}_\tau^1}{\partial A_\tau^t} $$
    *   其中 `W` 是一个掩码 (mask)，`β` 是作者引入的引导权重裁剪值，用于在控制任务常用的较少去噪步数下稳定算法。

3.  **软掩码 (Soft Masking)**:
    *   简单的硬掩码（即冻结部分权重为 1，其他为 0）在延迟 `d` 较小时不足以保证平滑过渡。
    *   作者提出了软掩码：
        *   对于前 `d` 个“冻结”动作，权重 `W_i` 为 1。
        *   对于介于冻结区和旧块末端之间的“中间区域”，权重从 1 指数衰减到 0。
        *   对于超出旧块范围的全新动作，权重为 0。
    *   这种平滑的权重衰减使得模型能够更自然地从遵循旧块的轨迹过渡到生成全新的动作。
    *   权重 `W_i` 的具体公式为：
        $$ W_i = \begin{cases} 1 & \text{if } i < d \\ c_i e^{\frac{i-1}{c_i-1}} & \text{if } d \le i < H-s \\ 0 & \text{if } i \ge H-s \end{cases} \quad \text{where } c_i = \frac{H-s-i}{H-s-d+1} $$

4.  **完整算法 (Algorithm 1)**:
    *   算法通过两个并发线程工作：一个**控制器线程**以固定频率调用 `GETACTION` 获取动作并发送给机器人；一个**推理线程**在后台运行 `INFERENCELOOP`。
    *   推理线程会根据最近的延迟估计 `d`，在执行了 `s` 个动作后，使用 `GUIDEDINFERENCE` 函数，结合当前观测 `o`、旧块中未执行的部分 `A_prev`，来生成新的动作块。

### 实验与结论

#### 实验设置
1.  **仿真实验**: 在新提出的 Kinetix 动态任务基准上进行。对比了 RTC (soft/hard masking)、Naive async (简单异步)、BID (双向解码) 和 TE (时间集成) 等方法在不同推理延迟 (`d=0` 到 `4`) 和不同执行步长 (`s`) 下的成功率。
2.  **真实世界实验**: 在一个双臂机器人平台上评估了 6 个复杂的操纵任务（如点燃火柴、叠衣服、整理床铺）。使用 `π_0.5` VLA 作为基础策略。对比了 RTC、Synchronous (同步，即等待推理)、TE-sparse 和 TE-dense。实验中还人为注入了 100ms 和 200ms 的额外延迟来测试鲁棒性。

#### 实验结论
1.  **RTC 性能优越且鲁棒**: 在仿真和真实世界中，RTC 的性能在各种延迟下都显著优于所有基线方法。特别是随着延迟增加，RTC 的优势愈发明显，而其他方法（尤其是 TE）性能急剧下降甚至完全失效。
2.  **吞吐量显著提升**: 在真实世界任务中，RTC 的平均任务吞吐量（综合了速度和成功率的指标）最高。即使在移除推理停顿时间后，RTC 完成任务的“纯执行时间”也比同步方法更短，这表明平滑的动作使其犯错更少，重试次数更少。
3.  **对精确任务至关重要**: 在“点燃火柴”这类对精度要求极高的任务中，RTC 展现出巨大的优势，成功率远高于其他方法，证明了平滑连续的动作对于完成精细操作至关重要。
4.  **软掩码优于硬掩码**: 仿真结果显示，软掩码在延迟较低时性能优于硬掩码，验证了其平滑过渡的有效性。

### 不足
1.  **计算开销**: RTC 在推理时需要进行反向传播来计算引导项的梯度，这比直接从策略中采样增加了额外的计算开销。
2.  **实验场景局限**: 虽然真实世界实验涵盖了多样的操作任务，但并未涉及如足式机器人行走等更具动态性的场景。这些场景可能会从实时执行中获益更多，是未来工作的方向。

## Insights

很多概念借鉴了 ICLR 等优秀会议的论文。比如 $\Pi \text{GDM}$ 等

## Ref and Tag