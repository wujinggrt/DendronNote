---
id: svc1e8p1tntyfmp9t9sygdn
title: DexVLA
desc: ''
updated: 1740052488893
created: 1740021087668
---

# DexVLA：面向通用机器人控制的插件式扩散专家VLM

[project web](https://dex-vla.github.io/)
[paper](https://arxiv.org/abs/2502.05855)

DexVLA 基于扩散的动作专家，可规模化到十亿个参数，为跨具身学习而设计。具身课程学习策略 (strategy) 促进有效的训练：(1) 从 VLA 分离扩散专家，并在跨具身数据上进行预训练这些专家；(2) 对齐 VLA 模型到特定具身；(3) 后训练以快速适应新任务。作者引入了实验，证明 DexVLA 能够在不同具身 (比如单臂、双臂和灵巧手) 上适应任务，不需要做任务适应的工作。

## 方法
![fig3](assets/images/robotics.DexVLA/fig3.png)
如图展示了 DexVLA 架构和具身课程学习。模型采用三阶段训练。阶段一，独立地训练扩散专家，不训练 VLM。阶段二三，继承扩散专家到 VLM，丢弃专家的视觉和语言组件。扩散专家使用多个头来跨具身学习。

### 模型架构
DexVLA 模型主要基于 Transformer 语言模型主干。遵循 VLM 模型的通用框架，用图像编码器将机器人的图像观察结果投影到与语言tokens相同的嵌入空间中，即 embedding_dim 相同。对于多个摄像机视图，拼接这些视觉 tokens。VLM 组件生成两个输出：推理 tokens 和动作 tokens。
* 动作 tokens 通过投影模块传递，该模块由两个具有 LayerNorm 的线性层组成。该模块类似于 LLaVA 等视觉-语言模型中设计的连接器，用于转换 VLM 的嵌入空间以符合动作专家的输入要求。
* 推理 tokens 使用 FiLM 层注入策略模型，这些层缩放和漂移策略内投影层的参数。因此，该模型可以自主生成推理并利用扩散专家中的这种推理来指导动作生成。

按照标准的后融合 (late fusion) VLM 配方 (LLaVA 1.5 使用了两层 MLP 作为 projector)，图像编码器将机器人的图像观测嵌入到与语言 tokens 相同的嵌入空间中。进一步用机器人特定的输入和输出（即本体感受状态和机器人动作）增强这个主干。

构建扩散专家。由于动作专家主导机器人动作的学习过程，因此设计良好的神经架构对于更好的视觉运动策略学习至关重要。用 Scale Diffusion Policy (ScaleDP [63])，它是 Transformer 架构中 Diffusion Policy 的变型，其中 ScaleDP 的最大版本最多有 1B 个参数。然而，简单的 ScaleDP 不是为跨具身预训练而设计的。本文设计一个多头输出，以便在具有各种机器人配置的 ScaleDP 上进行预训练。每个头负责一个机器人配置。此设置类似于 Octo。

训练目标。给定一批输入序列，整体训练损失定义为 ($L_{diff}$) 和下一个 token 预测 (NTP) 损失 ($L_{ntp}$) 的加权组合。

### 具身课程学习
课程学习 (curriculum learning) 是一种学习策略。三阶段训练策略实现了具身课程。第一阶段从跨具身数据学习可泛化运动技能，第二阶段适配特定物理形式，第三阶段改进特定任务行为。

精心设计的训练策略，对于优化深度神经网络至关重要。与网络固有训练动态相一致的方法可确保更高效、更有效的数据利用。框架 DexVLA 通过将视觉-语言模型 (VLM) 与扩散专家相结合，瞄准一般机器人控制。利用其模块化架构（结合两个不同的组件），提出一个三阶段训练策略，系统地解决：（1）学习灵巧的操作技能，使模型能够完成复杂的任务；（2）跨具身学习，使模型适应不同的机器人平台。

#### 阶段一：跨具身预训练
训练好的 VLM (比如 Qwen2-VL) 能够处理视觉和文本输入，但是缺乏与现实世界中不同目标交互的能力。为了有效训练动作专家，暂时将其与 VLM 组件分离 (就像鸡生蛋蛋生鸡需要先启动)。虽然 CNN 架构提取能力强大，但是 ViT 模型更契合 VLM 的主干 Transformer，所以用 ViT 架构做视频编码。语言嵌入使用 DistillBERT。再使用 FiLM 层将将生成的语言 embedding 集成到模型中。对扩散专家进行预训练的一个关键见解是，必须将长期任务（例如，收拾桌子、折叠衣物）分解为子任务。这些任务通常持续超过 2 分钟，扩散专家学习单一指令中有效学习是一个挑战。因此，在这些长期任务中标注子步骤指令，以提供更结构化的学习信号。使用子步骤进行预训练对于获得强大的性能至关重要。

相比 VLA，扩散专家参数更少，1B 参数，训练更快。

#### 阶段二：特定具身对齐
阶段一的跨具身预训练学习的内容可能损害在目标具身上的表现，所以需要特定具身数据训练模型来解决问题。对齐 VLM 中的抽象视觉语言表示与扩散专家。因此，过滤数据集以仅包含特定具身的数据，确保每个样本涉及单个具身。借鉴 LLaVA 一类的 VLM 技术，此阶段关注于对齐目标具身的动作空间，并且伴随所需的相机视图和跟随的语言指令。具体而言，联合训练 VLM 模型、投影层和扩散专家，使用了特定具身的数据，冻结 VLM 的视觉编码器。使得扩散专家有效的落地高层的 VL 理解到机器人特定的动机控制空间。

#### 阶段三：特定任务适配
改进模型熟练流畅地执行下游任务的能力，类似于大语言模型中的后训练阶段，在该阶段，模型根据特定域的数据进行微调。对于更简单、不太依赖泛化的任务，例如折叠衬衫、收拾桌子或使用训练的目标拾取箱子，由于模型已经表现良好，因此不需要进行特定任务的训练。但是，复杂、需要灵活性的任务需要模型学习细粒度、依赖于上下文的操作。因此，有效的后训练依赖于高质量的专家演示数据集，该数据集展示一致且流畅的任务执行策略，这些策略专注于促进成功完成任务的行为。注：在第 2 阶段和第 3 阶段都使用了带注释的子步骤语言数据。但是，不是直接使用这些子步骤推理作为指令输入，而是将它们用作中间语言输出，迫使模型学习和生成这些子步骤语言描述。这种方法已被证明非常有效，使模型能够执行复杂的长期任务，例如折叠衣物。虽然其他 VLA（如 π0）也可以完成此类任务，但它们依赖于高级策略模型（如 SayCan）来识别任务状态并提供下一步指示。相比之下，本文框架利用 VLM 主干作为隐式高级策略模型。这允许模型内部解释任务的状态并将这种理解注入策略以指导动作生成，从而无需外部高级策略模块。

#### 实施细节
使用了 Qwen-2-VL 作为基础 VLM。最多使用三个摄像机视图。扩散专家包含 10 亿个参数。两个 MLP 层（每个层后跟 LayerNorm）将视觉-语言模型连接到动作专家。用 FiLM 将视觉语言模型的最终嵌入注入扩散专家，并将其应用于两个 MLP 中的每一个。在第 1 阶段训练期间，用收集的数据集。第 2 阶段训练仅限于共享相同具身的数据。第 3 阶段后训练是有选择性的，仅针对明确提到的任务进行。预训练数据集共有 100 小时的数据，涉及 91 个不同的任务。

## 实验与评估
### Complex Long-Horizon Tasks with Direct Prompting
![fig4](assets/images/robotics.DexVLA/fig4_prompting_long_horizon.png.png)

在实验中，通过结合特定任务的后训练和自生成推理能力来解决一系列具有挑战性的多阶段任务。对于其中一些任务，数据在预训练中存在，但需要进行微调才能掌握。对于一些任务，预训练中没有数据。本次评估中的任务如图 4 所示。

## 见解
是否可以独立地，直接地模仿学习或强化学习。随后嵌入到 VLA 中。

思路就像训练 LLaVA 的视觉的 projector？

另外地，数据增强的方式，是否可以在执行后，收集数据，通过分割视频或图片中的机械臂和操作的物体，随后换背景图片，以此扩充训练数据，得到在不同场景下泛化和抗干扰的能力。

## Ref
ScaleDP arXiv:2409.14411, 2024.

## Tag
#Paper
#Robotics
#VLA